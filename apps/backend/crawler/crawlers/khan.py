import asyncio
import json
import logging
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum

import aiofiles
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Browser, Page

# Supabase Ïó∞Îèô import
sys.path.append(str(Path(__file__).parent.parent))
from apps.backend.app.services.article_service import ArticleService
from apps.backend.app.models.article import Article
from apps.backend.crawler.base import BaseNewsCrawler

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn
from rich.theme import Theme

console = Console(theme=Theme({
    "success": "bold green",
    "fail": "bold red",
    "info": "bold cyan"
}))

def print_status(msg, status="info"):
    console.print(msg, style=status)

# Î°úÍπÖ ÏÑ§Ï†ï
log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)
logging.basicConfig(
    level=logging.DEBUG,
    handlers=[
        logging.FileHandler(log_dir / "crawler_detailed.log", encoding="utf-8"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class Category(Enum):
    ECONOMY = "Í≤ΩÏ†ú"

@dataclass
class CrawlerConfig:
    # max_pages: int = 2  # ÌéòÏù¥ÏßÄ Ï†úÌïú ÏóÜÏùå, ÌïÑÏöîÏãú Î≥µÍµ¨
    articles_per_category: int = 30  # Ïã§Ï†Ñ ÌÖåÏä§Ìä∏Î•º ÏúÑÌï¥ 30Í∞úÎ°ú Î™ÖÏãú
    page_timeout: int = 20000
    wait_timeout: int = 2000
    min_content_length: int = 30
    min_title_length: int = 10
    # Í≤ΩÌñ•Ïã†Î¨∏Ïö© ÏÖÄÎ†âÌÑ∞
    list_selector: str = 'div.list ul#recentList > li > article > div > a'
    title_selector: str = 'article > header > h1'
    content_selector: str = 'section.art_cont div.art_body#articleBody'
    date_selector: str = 'article > header > div.date > p'
    author_selector: str = 'article > header > ul.bottom > li.editor > a'
    image_selector: str = 'section.art_cont div.art_body#articleBody img'

class ConsoleUI:
    @staticmethod
    def print_header():
        console.rule("[bold blue]üöÄ Í≤ΩÌñ•Ïã†Î¨∏ ÌÅ¨Î°§Îü¨ ÏãúÏûë")
    @staticmethod
    def print_category_start(category: str):
        console.print(f"[bold cyan]\nüì∞ {category} Ïπ¥ÌÖåÍ≥†Î¶¨ ÌÅ¨Î°§ÎßÅ Ï§ë...[/bold cyan]")
    @staticmethod
    def print_category_complete(category: str, count: int):
        console.print(f"[bold green]‚úÖ {category}: {count}Í∞ú Í∏∞ÏÇ¨ ÏàòÏßë ÏôÑÎ£å[/bold green]")
    @staticmethod
    def print_summary(total_articles: int, filepath: str):
        console.rule("[bold magenta]üéâ ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å!")
        console.print(f"[bold yellow]üìä Ï¥ù {total_articles}Í∞ú Í∏∞ÏÇ¨ ÏàòÏßë")
        console.print(f"[bold yellow]üíæ Ï†ÄÏû• ÏúÑÏπò: {filepath}")
        console.rule()

class ArticleExtractor:
    def __init__(self, config: CrawlerConfig):
        self.config = config

    async def extract_article_content(self, page: Page, url: str) -> Optional[Dict[str, Any]]:
        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=self.config.page_timeout)
            await page.wait_for_timeout(1500)
            html = await page.content()
            soup = BeautifulSoup(html, 'html.parser')
            # Ï†úÎ™©
            title = soup.select_one(self.config.title_selector)
            title = title.get_text(strip=True) if title else None
            if not title or len(title) < self.config.min_title_length:
                logger.debug(f"Ï†úÎ™© ÏóÜÏùå: {url}")
                return None
            # Î≥∏Î¨∏
            content = ''
            content_area = soup.select_one(self.config.content_selector)
            if content_area:
                # Í¥ëÍ≥†/Î∞∞ÎÑà Îì± Î∂àÌïÑÏöî ÌÉúÍ∑∏ Ï†úÍ±∞
                for tag in content_area.find_all(['script', 'style', 'iframe', 'div', 'aside', 'footer', 'form', 'nav', 'button', 'noscript', 'svg', 'figure', 'figcaption', 'ins', 'ul', 'li'], recursive=True):
                    tag.decompose()
                content = '\n'.join([p.get_text(" ", strip=True) for p in content_area.find_all(['p', 'span'])])
            if not content or len(content.strip()) < self.config.min_content_length:
                logger.debug(f"Î≥∏Î¨∏ ÏóÜÏùå: {url}")
                return None
            # Î∞úÌñâÏùº
            date = soup.select_one(self.config.date_selector)
            published_at = None
            if date:
                date_text = date.get_text(strip=True)
                # 'ÏûÖÎ†• 2025.07.14 18:00' Îì±ÏóêÏÑú ÎÇ†ÏßúÎßå Ï∂îÏ∂ú
                m = re.search(r'(\d{4}\.\d{2}\.\d{2} \d{2}:\d{2})', date_text)
                if m:
                    published_at = datetime.strptime(m.group(1), "%Y.%m.%d %H:%M").isoformat()
            # ÌòπÏãúÎùºÎèÑ Î™ª Ï∞æÏúºÎ©¥ Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ÏóêÏÑú Ìïú Î≤à Îçî ÏãúÎèÑ
            if not published_at:
                all_text = soup.get_text()
                m = re.search(r'(\d{4}\.\d{2}\.\d{2} \d{2}:\d{2})', all_text)
                if m:
                    published_at = datetime.strptime(m.group(1), "%Y.%m.%d %H:%M").isoformat()
            # Í∏∞ÏûêÎ™Ö
            author = soup.select_one(self.config.author_selector)
            author = author.get_text(strip=True) if author else ''
            # Ïù¥ÎØ∏ÏßÄ
            image = soup.select_one(self.config.image_selector)
            image_url = image['src'] if image and image.has_attr('src') else ''
            return {
                "title": title,
                "url": url,
                "content_full": content,
                "published_at": published_at or datetime.now().isoformat(),
                "author": author,
                "image_url": image_url
            }
        except Exception as e:
            logger.debug(f"Í∏∞ÏÇ¨ Ï∂îÏ∂ú Ïã§Ìå® {url}: {str(e)}")
            return None

# --- Í≥µÌÜµ Ïú†Ìã∏: Ïñ∏Î°†ÏÇ¨Î™ÖÏúºÎ°ú media_id, bias ÏïàÏ†ÑÌïòÍ≤å Í∞ÄÏ†∏Ïò§Í∏∞ ---
import requests, os

def get_media_info(media_name: str):
    default_map = {
        'Í≤ΩÌñ•Ïã†Î¨∏': {
            'media_id': '4a870e44-6fb5-467f-b236-da3687affbff',
            'bias': 'left',
        },
    }
    SUPABASE_URL = os.getenv("SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_KEY")
    if SUPABASE_URL and SUPABASE_KEY:
        headers = {
            "apikey": SUPABASE_KEY,
            "Authorization": f"Bearer {SUPABASE_KEY}"
        }
        url = f"{SUPABASE_URL}/rest/v1/media_outlets?name=eq.{media_name}"
        try:
            resp = requests.get(url, headers=headers, timeout=5)
            if resp.status_code == 200 and resp.json():
                media = resp.json()[0]
                media_id = media.get("id") or default_map.get(media_name, {}).get("media_id", "")
                bias = media.get("bias") or default_map.get(media_name, {}).get("bias", "")
                return media_id, bias
        except Exception:
            pass
    d = default_map.get(media_name, {})
    return d.get("media_id", ""), d.get("bias", "")

class KhanCrawler(BaseNewsCrawler):
    CATEGORY_URLS = {
        Category.ECONOMY: "https://www.khan.co.kr/economy"
    }
    def __init__(self, config: CrawlerConfig):
        super().__init__()
        self.config = config
        self.extractor = ArticleExtractor(config)
        self.article_service = ArticleService()
        self.visited_urls = set()

    async def crawl_category(self, browser: Browser, category: Category) -> List[Dict[str, Any]]:
        category_base_url = self.CATEGORY_URLS[category]
        articles = []
        page = await browser.new_page()
        try:
            page_num = 1
            while len(articles) < self.config.articles_per_category:
                list_url = f"{category_base_url}?page={page_num}" if page_num > 1 else category_base_url
                await page.goto(list_url, wait_until="domcontentloaded", timeout=self.config.page_timeout)
                # Í∏∞ÏÇ¨ Î¶¨Ïä§Ìä∏Í∞Ä Î°úÎìúÎê† ÎïåÍπåÏßÄ Î™ÖÏãúÏ†ÅÏúºÎ°ú ÎåÄÍ∏∞
                try:
                    await page.wait_for_selector(self.config.list_selector, timeout=5000)
                except Exception:
                    logger.warning(f"Í∏∞ÏÇ¨ Î¶¨Ïä§Ìä∏ Î°úÎìú Ïã§Ìå®: {list_url}")
                html = await page.content()
                soup = BeautifulSoup(html, 'html.parser')
                article_links = []
                for a in soup.select(self.config.list_selector):
                    href = a.get('href')
                    if isinstance(href, list):
                        href = href[0] if href else None
                    if href and isinstance(href, str):
                        if href.startswith('/'):
                            href = f'https://www.khan.co.kr{href}'
                        if href.startswith('http') and href not in self.visited_urls:
                            article_links.append(href)
                            self.visited_urls.add(href)
                if not article_links:
                    # ÎîîÎ≤ÑÍπÖÏö©: HTML Ï†ÄÏû•
                    with open(f"debug_khan_page_{page_num}.html", "w", encoding="utf-8") as f:
                        f.write(html)
                    break  # Îçî Ïù¥ÏÉÅ Í∏∞ÏÇ¨Í∞Ä ÏóÜÏúºÎ©¥ Ï¢ÖÎ£å
                for url in article_links:
                    if len(articles) >= self.config.articles_per_category:
                        break
                    article_data = await self.extractor.extract_article_content(page, url)
                    if article_data:
                        article_data['category'] = category.value
                        article_data['source'] = 'Í≤ΩÌñ•Ïã†Î¨∏'
                        articles.append(article_data)
                        print_status(f"‚úî {article_data['title'][:40]} ... ÏÑ±Í≥µ", "success")
                    else:
                        print_status(f"‚úñ {url} ... Î≥∏Î¨∏ ÏóÜÏùå", "fail")
                page_num += 1
        finally:
            await page.close()
        return articles

    async def crawl_all_categories(self, test_mode: bool = False) -> List[Dict[str, Any]]:
        ConsoleUI.print_header()
        all_articles = []
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            try:
                for category in Category:
                    ConsoleUI.print_category_start(category.value)
                    articles = await self.crawl_category(browser, category)
                    all_articles.extend(articles)
                    ConsoleUI.print_category_complete(category.value, len(articles))
                    if test_mode:
                        break
            finally:
                await browser.close()
        return all_articles

    async def save_articles(self, articles: List[Dict[str, Any]]) -> str:
        if not articles:
            logger.warning("Ï†ÄÏû•Ìï† Í∏∞ÏÇ¨Í∞Ä ÏóÜÏäµÎãàÎã§.")
            return ""
        data_dir = Path("data/raw")
        data_dir.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"khan_articles_{timestamp}.jsonl"
        filepath = data_dir / filename
        async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
            for article in articles:
                await f.write(json.dumps(article, ensure_ascii=False) + '\n')
        return str(filepath)

    async def save_articles_to_db(self, articles: List[Dict[str, Any]]) -> int:
        if not articles:
            logger.warning("Ï†ÄÏû•Ìï† Í∏∞ÏÇ¨Í∞Ä ÏóÜÏäµÎãàÎã§.")
            return 0
        try:
            article_objects = []
            for article_data in articles:
                media_id, bias = get_media_info('Í≤ΩÌñ•Ïã†Î¨∏')
                article = Article(
                    title=article_data.get('title', ''),
                    url=article_data.get('url', ''),
                    category=article_data.get('category', ''),
                    content_full=article_data.get('content_full', ''),
                    published_at=None,
                    author=article_data.get('author', ''),
                    image_url=article_data.get('image_url', ''),
                    media_id=media_id,
                    bias=bias
                )
                article_objects.append(article)
            saved_count = await self.article_service.save_articles(article_objects)
            return saved_count
        except Exception as e:
            logger.error(f"DB Ï†ÄÏû• Ïã§Ìå®: {e}")
            return 0

async def main():
    config = CrawlerConfig()
    crawler = KhanCrawler(config)
    media_id, bias = get_media_info('Í≤ΩÌñ•Ïã†Î¨∏')
    articles = await crawler.crawl_all_categories()
    if articles:
        for article in articles:
            article["media_id"] = media_id
            article["bias"] = bias
        filepath = await crawler.save_articles(articles)
        await crawler.save_articles_to_db(articles)
        ConsoleUI.print_summary(len(articles), filepath)
    else:
        print("ÌÅ¨Î°§ÎßÅÎêú Í∏∞ÏÇ¨Í∞Ä ÏóÜÏäµÎãàÎã§.")

if __name__ == "__main__":
    asyncio.run(main())
