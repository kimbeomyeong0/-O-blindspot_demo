import asyncio
import json
import logging
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum

import aiofiles
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Browser, Page

# Supabase ì—°ë™ import
sys.path.append(str(Path(__file__).parent.parent))
from apps.backend.app.services.article_service import ArticleService
from apps.backend.app.models.article import Article
from apps.backend.crawler.base import BaseNewsCrawler

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn
from rich.theme import Theme

console = Console(theme=Theme({
    "success": "bold green",
    "fail": "bold red",
    "info": "bold cyan"
}))

def print_status(msg, status="info"):
    console.print(msg, style=status)

# ë¡œê¹… ì„¤ì •
log_dir = Path("logs")
log_dir.mkdir(exist_ok=True)
logging.basicConfig(
    level=logging.DEBUG,
    handlers=[
        logging.FileHandler(log_dir / "crawler_detailed.log", encoding="utf-8"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class Category(Enum):
    ECONOMY = "ê²½ì œ"

@dataclass
class CrawlerConfig:
    # max_pages: int = 2  # í˜ì´ì§€ ì œí•œ ì—†ìŒ, í•„ìš”ì‹œ ë³µêµ¬
    articles_per_category: int = 30  # ì‹¤ì „ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 30ê°œë¡œ ëª…ì‹œ
    page_timeout: int = 20000
    wait_timeout: int = 2000
    min_content_length: int = 30
    min_title_length: int = 10
    # ê²½í–¥ì‹ ë¬¸ìš© ì…€ë ‰í„°
    list_selector: str = 'div.list ul#recentList > li > article > div > a'
    title_selector: str = 'article > header > h1'
    content_selector: str = 'section.art_cont div.art_body#articleBody'
    date_selector: str = 'article > header > div.date > p'
    author_selector: str = 'article > header > ul.bottom > li.editor > a'
    image_selector: str = 'section.art_cont div.art_body#articleBody img'

class ConsoleUI:
    @staticmethod
    def print_header():
        console.rule("[bold blue]ğŸš€ ê²½í–¥ì‹ ë¬¸ í¬ë¡¤ëŸ¬ ì‹œì‘")
    @staticmethod
    def print_category_start(category: str):
        console.print(f"[bold cyan]\nğŸ“° {category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¤‘...[/bold cyan]")
    @staticmethod
    def print_category_complete(category: str, count: int):
        console.print(f"[bold green]âœ… {category}: {count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ[/bold green]")
    @staticmethod
    def print_summary(total_articles: int, filepath: str):
        console.rule("[bold magenta]ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        console.print(f"[bold yellow]ğŸ“Š ì´ {total_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        console.print(f"[bold yellow]ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {filepath}")
        console.rule()

class ArticleExtractor:
    def __init__(self, config: CrawlerConfig):
        self.config = config

    async def extract_article_content(self, page: Page, url: str) -> Optional[Dict[str, Any]]:
        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=self.config.page_timeout)
            await page.wait_for_timeout(1500)
            html = await page.content()
            soup = BeautifulSoup(html, 'html.parser')
            # ì œëª©
            title = soup.select_one(self.config.title_selector)
            title = title.get_text(strip=True) if title else None
            if not title or len(title) < self.config.min_title_length:
                logger.debug(f"ì œëª© ì—†ìŒ: {url}")
                return None
            # ë³¸ë¬¸
            content = ''
            content_area = soup.select_one(self.config.content_selector)
            if content_area:
                # ê´‘ê³ /ë°°ë„ˆ ë“± ë¶ˆí•„ìš” íƒœê·¸ ì œê±°
                for tag in content_area.find_all(['script', 'style', 'iframe', 'div', 'aside', 'footer', 'form', 'nav', 'button', 'noscript', 'svg', 'figure', 'figcaption', 'ins', 'ul', 'li'], recursive=True):
                    tag.decompose()
                content = '\n'.join([p.get_text(" ", strip=True) for p in content_area.find_all(['p', 'span'])])
            if not content or len(content.strip()) < self.config.min_content_length:
                logger.debug(f"ë³¸ë¬¸ ì—†ìŒ: {url}")
                return None
            # ë°œí–‰ì¼
            date = soup.select_one(self.config.date_selector)
            published_at = None
            if date:
                date_text = date.get_text(strip=True)
                # 'ì…ë ¥ 2025.07.14 18:00' ë“±ì—ì„œ ë‚ ì§œë§Œ ì¶”ì¶œ
                m = re.search(r'(\d{4}\.\d{2}\.\d{2} \d{2}:\d{2})', date_text)
                if m:
                    published_at = datetime.strptime(m.group(1), "%Y.%m.%d %H:%M").isoformat()
            # í˜¹ì‹œë¼ë„ ëª» ì°¾ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ í•œ ë²ˆ ë” ì‹œë„
            if not published_at:
                all_text = soup.get_text()
                m = re.search(r'(\d{4}\.\d{2}\.\d{2} \d{2}:\d{2})', all_text)
                if m:
                    published_at = datetime.strptime(m.group(1), "%Y.%m.%d %H:%M").isoformat()
            # ê¸°ìëª…
            author = soup.select_one(self.config.author_selector)
            author = author.get_text(strip=True) if author else ''
            # ì´ë¯¸ì§€
            image = soup.select_one(self.config.image_selector)
            image_url = image['src'] if image and image.has_attr('src') else ''
            return {
                "title": title,
                "url": url,
                "content_full": content,
                "published_at": published_at or datetime.now().isoformat(),
                "author": author,
                "image_url": image_url
            }
        except Exception as e:
            logger.debug(f"ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {str(e)}")
            return None

# --- ê³µí†µ ìœ í‹¸: ì–¸ë¡ ì‚¬ëª…ìœ¼ë¡œ media_id, bias ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸° ---
import requests, os

def get_media_info(media_name: str):
    default_map = {
        'ê²½í–¥ì‹ ë¬¸': {
            'media_id': '4a870e44-6fb5-467f-b236-da3687affbff',
            'bias': 'left',
        },
    }
    SUPABASE_URL = os.getenv("SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_KEY")
    if SUPABASE_URL and SUPABASE_KEY:
        headers = {
            "apikey": SUPABASE_KEY,
            "Authorization": f"Bearer {SUPABASE_KEY}"
        }
        url = f"{SUPABASE_URL}/rest/v1/media_outlets?name=eq.{media_name}"
        try:
            resp = requests.get(url, headers=headers, timeout=5)
            if resp.status_code == 200 and resp.json():
                media = resp.json()[0]
                media_id = media.get("id") or default_map.get(media_name, {}).get("media_id", "")
                bias = media.get("bias") or default_map.get(media_name, {}).get("bias", "")
                return media_id, bias
        except Exception:
            pass
    d = default_map.get(media_name, {})
    return d.get("media_id", ""), d.get("bias", "")

class KhanCrawler(BaseNewsCrawler):
    CATEGORY_URLS = {
        Category.ECONOMY: "https://www.khan.co.kr/economy"
    }
    def __init__(self, config: CrawlerConfig):
        super().__init__()
        self.config = config
        self.extractor = ArticleExtractor(config)
        self.article_service = ArticleService()
        self.visited_urls = set()

    async def crawl_category(self, browser: Browser, category: Category) -> List[Dict[str, Any]]:
        category_base_url = self.CATEGORY_URLS[category]
        articles = []
        page = await browser.new_page()
        try:
            page_num = 1
            while len(articles) < self.config.articles_per_category:
                list_url = f"{category_base_url}?page={page_num}" if page_num > 1 else category_base_url
                await page.goto(list_url, wait_until="domcontentloaded", timeout=self.config.page_timeout)
                # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëª…ì‹œì ìœ¼ë¡œ ëŒ€ê¸°
                try:
                    await page.wait_for_selector(self.config.list_selector, timeout=5000)
                except Exception:
                    logger.warning(f"ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {list_url}")
                html = await page.content()
                soup = BeautifulSoup(html, 'html.parser')
                article_links = []
                for a in soup.select(self.config.list_selector):
                    href = a.get('href')
                    if isinstance(href, list):
                        href = href[0] if href else None
                    if href and isinstance(href, str):
                        if href.startswith('/'):
                            href = f'https://www.khan.co.kr{href}'
                        if href.startswith('http') and href not in self.visited_urls:
                            article_links.append(href)
                            self.visited_urls.add(href)
                if not article_links:
                    # ë””ë²„ê¹…ìš©: HTML ì €ì¥
                    with open(f"debug_khan_page_{page_num}.html", "w", encoding="utf-8") as f:
                        f.write(html)
                    break  # ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                for url in article_links:
                    if len(articles) >= self.config.articles_per_category:
                        break
                    article_data = await self.extractor.extract_article_content(page, url)
                    if article_data:
                        article_data['category'] = category.value
                        article_data['source'] = 'ê²½í–¥ì‹ ë¬¸'
                        articles.append(article_data)
                        print_status(f"âœ” {article_data['title'][:40]} ... ì„±ê³µ", "success")
                    else:
                        print_status(f"âœ– {url} ... ë³¸ë¬¸ ì—†ìŒ", "fail")
                page_num += 1
        finally:
            await page.close()
        return articles

    async def crawl_all_categories(self, test_mode: bool = False) -> List[Dict[str, Any]]:
        ConsoleUI.print_header()
        all_articles = []
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            try:
                for category in Category:
                    ConsoleUI.print_category_start(category.value)
                    articles = await self.crawl_category(browser, category)
                    all_articles.extend(articles)
                    ConsoleUI.print_category_complete(category.value, len(articles))
                    if test_mode:
                        break
            finally:
                await browser.close()
        return all_articles

    async def save_articles(self, articles: List[Dict[str, Any]]) -> str:
        if not articles:
            logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return ""
        data_dir = Path("data/raw")
        data_dir.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"khan_articles_{timestamp}.jsonl"
        filepath = data_dir / filename
        async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
            for article in articles:
                await f.write(json.dumps(article, ensure_ascii=False) + '\n')
        return str(filepath)

    async def save_articles_to_db(self, articles: List[Dict[str, Any]]) -> int:
        if not articles:
            logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        try:
            article_objects = []
            for article_data in articles:
                media_id, bias = get_media_info('ê²½í–¥ì‹ ë¬¸')
                article = Article(
                    title=article_data.get('title', ''),
                    url=article_data.get('url', ''),
                    category=article_data.get('category', ''),
                    content_full=article_data.get('content_full', ''),
                    published_at=None,
                    author=article_data.get('author', ''),
                    image_url=article_data.get('image_url', ''),
                    media_id=media_id,
                    bias=bias
                )
                article_objects.append(article)
            saved_count = await self.article_service.save_articles(article_objects)
            return saved_count
        except Exception as e:
            logger.error(f"DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return 0

async def main():
    config = CrawlerConfig()
    crawler = KhanCrawler(config)
    media_id, bias = get_media_info('ê²½í–¥ì‹ ë¬¸')
    articles = await crawler.crawl_all_categories()
    if articles:
        for article in articles:
            article["media_id"] = media_id
            article["bias"] = bias
        filepath = await crawler.save_articles(articles)
        await crawler.save_articles_to_db(articles)
        ConsoleUI.print_summary(len(articles), filepath)
    else:
        print("í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
