import asyncio
import json
import logging
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Set, Tuple, Any
from dataclasses import dataclass
from enum import Enum

import aiofiles
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Browser, Page

# Supabase ì—°ë™ì„ ìœ„í•œ import
import sys
sys.path.append(str(Path(__file__).parent.parent))
from apps.backend.app.services.article_service import ArticleService
from apps.backend.app.models.article import Article
from apps.backend.crawler.base import BaseNewsCrawler

# ë¡œê¹… ì„¤ì • - íŒŒì¼ê³¼ ì½˜ì†” ë¶„ë¦¬
def setup_logging():
    """ë¡œê¹… ì„¤ì •ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # íŒŒì¼ ë¡œê±° (ìƒì„¸ ì •ë³´)
    file_handler = logging.FileHandler(log_dir / "crawler_detailed.log", encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)
    file_formatter = logging.Formatter(
        "%(asctime)s %(name)s %(levelname)s %(message)s"
    )
    file_handler.setFormatter(file_formatter)
    
    # ì½˜ì†” ë¡œê±° (ê°„ë‹¨í•œ ì •ë³´ë§Œ)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter("%(message)s")
    console_handler.setFormatter(console_formatter)
    
    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    logging.basicConfig(
        level=logging.DEBUG,
        handlers=[file_handler, console_handler]
    )

setup_logging()
logger = logging.getLogger(__name__)

class Category(Enum):
    """í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ ì •ì˜"""
    POLITICS = "ì •ì¹˜"
    ECONOMY = "ê²½ì œ"
    NATIONAL = "ì‚¬íšŒ"
    INTERNATIONAL = "êµ­ì œ"
    SPORTS = "ìŠ¤í¬ì¸ "
    CULTURE = "ë¬¸í™”"
    ENTERTAINMENT = "ì—°ì˜ˆ"

@dataclass
class CrawlerConfig:
    """í¬ë¡¤ëŸ¬ ì„¤ì • í´ë˜ìŠ¤"""
    max_more_clicks: int = 10
    articles_per_category: int = 30
    page_timeout: int = 20000
    wait_timeout: int = 2000
    min_content_length: int = 50
    min_title_length: int = 10
    
    # CSS ì…€ë ‰í„°ë“¤
    headline_selector: str = 'a[href*="/article/"]'
    title_selectors: Optional[List[str]] = None
    content_selectors: Optional[List[str]] = None
    date_selectors: Optional[List[str]] = None
    author_selectors: Optional[List[str]] = None
    image_selectors: Optional[List[str]] = None
    
    def __post_init__(self):
        if self.title_selectors is None:
            self.title_selectors = ['h1', '.article-title', '.title', '[class*="title"]', 'h2']
        if self.content_selectors is None:
            self.content_selectors = [
                '.article-content', '.content', '[class*="content"]', 
                '.article-body', '.body', 'article', '.article-text'
            ]
        if self.date_selectors is None:
            self.date_selectors = [
                ".date", ".time", "[class*='date']", "[class*='time']", 
                ".published", ".article-date"
            ]
        if self.author_selectors is None:
            self.author_selectors = [
                ".author", ".byline", ".writer", ".reporter", "[class*='author']"
            ]
        if self.image_selectors is None:
            self.image_selectors = [
                ".article-image img", ".content img", "article img", ".thumb-img"
            ]

class ConsoleUI:
    """í„°ë¯¸ë„ UI ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def print_header():
        """í¬ë¡¤ë§ ì‹œì‘ í—¤ë”ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print("\n" + "="*60)
        print("ğŸš€ JTBC ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì‹œì‘")
        print("="*60)
    
    @staticmethod
    def print_category_start(category: str):
        """ì¹´í…Œê³ ë¦¬ ì‹œì‘ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print(f"\nğŸ“° {category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¤‘...")
    
    @staticmethod
    def print_progress(category: str, current: int, target: int, total_articles: int):
        """ì§„í–‰ ìƒí™©ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        progress = min(100, int(current / target * 100))
        bar = "â–ˆ" * (progress // 5) + "â–‘" * (20 - progress // 5)
        print(f"   {bar} {current}/{target} ({progress}%) - ì´ {total_articles}ê°œ ê¸°ì‚¬")
    
    @staticmethod
    def print_category_complete(category: str, count: int):
        """ì¹´í…Œê³ ë¦¬ ì™„ë£Œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print(f"âœ… {category}: {count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
    
    @staticmethod
    def print_summary(total_articles: int, filepath: str):
        """ìµœì¢… ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print("\n" + "="*60)
        print(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ {total_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {filepath}")
        print("="*60 + "\n")

class ArticleExtractor:
    """ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ í´ë˜ìŠ¤"""
    
    def __init__(self, config: CrawlerConfig):
        self.config = config
    
    async def extract_article_content(self, page: Page, url: str) -> Optional[Dict[str, Any]]:
        """ê¸°ì‚¬ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=self.config.page_timeout)
            await page.wait_for_timeout(500)
            
            html = await page.content()
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = self._extract_title(soup)
            if not title:
                logger.debug(f"ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}")
                return None
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = self._extract_content(soup)
            if not content or len(content.strip()) < self.config.min_content_length:
                logger.debug(f"ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}")
                return None
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            published_at = self._extract_published_date(soup)
            author = self._extract_author(soup)
            image_url = self._extract_image_url(soup)
            
            return {
                "title": title,
                "url": url,
                "content_full": content,
                "published_at": published_at or datetime.now().isoformat(),
                "author": author,
                "image_url": image_url
            }
            
        except Exception as e:
            logger.debug(f"ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {str(e)}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if self.config.title_selectors is None:
            return None
        for selector in self.config.title_selectors:
            el = soup.select_one(selector)
            if el:
                title = el.get_text(strip=True)
                if len(title) >= self.config.min_title_length:
                    return title
        return None
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if self.config.content_selectors is None:
            return ""
        
        for selector in self.config.content_selectors:
            el = soup.select_one(selector)
            if el:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for unwanted in el.find_all(['script', 'style', 'nav', 'header', 'footer']):
                    unwanted.decompose()
                
                content = el.get_text(strip=True)
                if len(content) >= self.config.min_content_length:
                    return content
        return ""
    
    def _extract_published_date(self, soup: BeautifulSoup) -> Optional[str]:
        """ë°œí–‰ì¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if self.config.date_selectors is None:
            return None
        
        for selector in self.config.date_selectors:
            el = soup.select_one(selector)
            if el:
                date_text = el.get_text(strip=True)
                # ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
                date_match = re.search(r'(\d{4})[.-](\d{1,2})[.-](\d{1,2})', date_text)
                if date_match:
                    return f"{date_match.group(1)}-{date_match.group(2).zfill(2)}-{date_match.group(3).zfill(2)}"
        return None
    
    def _extract_author(self, soup: BeautifulSoup) -> Optional[str]:
        """ê¸°ìë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if self.config.author_selectors is None:
            return None
        
        for selector in self.config.author_selectors:
            el = soup.select_one(selector)
            if el:
                author = el.get_text(strip=True)
                if author and len(author) > 0:
                    return author
        return None
    
    def _extract_image_url(self, soup: BeautifulSoup) -> Optional[str]:
        """ì´ë¯¸ì§€ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if self.config.image_selectors is None:
            return None
        
        for selector in self.config.image_selectors:
            el = soup.select_one(selector)
            if el and el.get('src'):
                image_url = el.get('src')
                if image_url and not image_url.startswith('data:'):
                    return image_url
        return None

class JTBCNewsCrawler(BaseNewsCrawler):
    """JTBC ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""
    
    CATEGORY_URLS = {
        Category.POLITICS: "https://news.jtbc.co.kr/sections/10",
        Category.ECONOMY: "https://news.jtbc.co.kr/sections/20",
        Category.NATIONAL: "https://news.jtbc.co.kr/sections/30",
        Category.INTERNATIONAL: "https://news.jtbc.co.kr/sections/40",
        Category.SPORTS: "https://news.jtbc.co.kr/sections/70",
        Category.CULTURE: "https://news.jtbc.co.kr/sections/50",
        Category.ENTERTAINMENT: "https://news.jtbc.co.kr/sections/60"
    }
    
    def __init__(self, config: CrawlerConfig):
        super().__init__()
        self.config = config
        self.extractor = ArticleExtractor(config)
        self.article_service = ArticleService()
        self.media_id = None  # JTBC media_id (DBì—ì„œ ì¡°íšŒ í•„ìš”)
        self.bias = 'left'    # JTBC bias (DBì—ì„œ ì¡°íšŒ í•„ìš”)
        self.base_url = "https://news.jtbc.co.kr"
    
    async def crawl_category(self, browser: Browser, category: Category) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ê¸°ì‚¬ë“¤ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        ConsoleUI.print_category_start(category.value)
        
        page = await browser.new_page()
        articles = []
        
        try:
            # ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ë¡œ ì´ë™
            category_url = self.CATEGORY_URLS[category]
            await page.goto(category_url, wait_until="networkidle")
            
            # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ìœ¼ë¡œ ë” ë§ì€ ê¸°ì‚¬ ë¡œë“œ
            await self._load_more_articles(page, category)
            
            # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
            links_and_titles = await self._extract_article_links(page, category)
            
            if not links_and_titles:
                logger.warning(f"{category.value}: ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            # ê¸°ì‚¬ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
            articles = await self._extract_articles(page, category, links_and_titles)
            
            ConsoleUI.print_category_complete(category.value, len(articles))
            return articles
            
        except Exception as e:
            logger.error(f"{category.value} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
        finally:
            await page.close()
    
    async def _load_more_articles(self, page: Page, category: Category) -> None:
        """ë”ë³´ê¸° ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë” ë§ì€ ê¸°ì‚¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        for i in range(self.config.max_more_clicks):
            try:
                # ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸°
                more_button = page.locator("button:has-text('ë”ë³´ê¸°')")
                if await more_button.count() > 0:
                    await more_button.click()
                    await page.wait_for_timeout(self.config.wait_timeout)
                    logger.debug(f"ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ {i+1}íšŒ")
                else:
                    break
            except Exception as e:
                logger.debug(f"ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}")
                break
    
    async def _get_current_article_links(self, page: Page) -> List[Tuple[str, str]]:
        """í˜„ì¬ í˜ì´ì§€ì˜ ê¸°ì‚¬ ë§í¬ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        links_and_titles = []
        seen_links = set()
        
        try:
            # ê¸°ì‚¬ ë§í¬ ìš”ì†Œë“¤ ì°¾ê¸°
            link_elements = await page.locator(self.config.headline_selector).all()
            
            for element in link_elements:
                try:
                    href = await element.get_attribute("href")
                    if href:
                        # ì „ì²´ URLë¡œ ë³€í™˜
                        full_url = self.base_url + href if href.startswith("/") else href
                        
                        # ì¤‘ë³µ ì²´í¬ ë° ê¸°ì‚¬ URL íŒ¨í„´ í™•ì¸
                        if (full_url not in seen_links and 
                            "/article/" in full_url and
                            "NB" in full_url):  # JTBC ê¸°ì‚¬ ID íŒ¨í„´
                            
                            # ì œëª© ì¶”ì¶œ ì‹œë„
                            title = await element.text_content()
                            title = title.strip() if title else ""
                            
                            links_and_titles.append((full_url, title))
                            seen_links.add(full_url)
                            
                except Exception as e:
                    logger.debug(f"ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
            
        except Exception as e:
            logger.error(f"ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return links_and_titles
    
    async def _extract_article_links(self, page: Page, category: Category) -> List[Tuple[str, str]]:
        """ê¸°ì‚¬ ë§í¬ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        links_and_titles = await self._get_current_article_links(page)
        
        # ì¤‘ë³µ ì œê±° ë° ì œí•œ
        unique_links = []
        seen_urls = set()
        
        for url, title in links_and_titles:
            if url not in seen_urls:
                unique_links.append((url, title))
                seen_urls.add(url)
                
                if len(unique_links) >= self.config.articles_per_category:
                    break
        
        logger.info(f"{category.value}: {len(unique_links)}ê°œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘")
        return unique_links
    
    async def _extract_articles(self, page: Page, category: Category, links_and_titles: List[Tuple[str, str]]) -> List[Dict[str, Any]]:
        """ê¸°ì‚¬ ìƒì„¸ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        articles = []
        
        for i, (url, title) in enumerate(links_and_titles, 1):
            ConsoleUI.print_progress(category.value, i, len(links_and_titles), len(articles))
            
            try:
                article_data = await self.extractor.extract_article_content(page, url)
                if article_data:
                    article_data["category"] = category.value
                    articles.append(article_data)
                
                # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.debug(f"ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {e}")
                continue
        
        return articles
    
    async def crawl_all_categories(self, test_mode: bool = False) -> List[Dict[str, Any]]:
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        ConsoleUI.print_header()
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            
            try:
                all_articles = []
                
                for category in Category:
                    if test_mode and len(all_articles) >= 5:  # í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” 5ê°œë§Œ
                        break
                    
                    articles = await self.crawl_category(browser, category)
                    all_articles.extend(articles)
                
                # ê²°ê³¼ ì €ì¥
                if all_articles:
                    await self.save_articles_to_db(all_articles)
                
                return all_articles
                
            except Exception as e:
                logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return []
            finally:
                await browser.close()
    
    async def save_articles(self, articles: List[Dict[str, Any]]) -> str:
        """ê¸°ì‚¬ë“¤ì„ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"jtbc_articles_{timestamp}.json"
        
        async with aiofiles.open(filename, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(articles, ensure_ascii=False, indent=2))
        
        return filename
    
    async def save_articles_to_db(self, articles: List[Dict[str, Any]]) -> int:
        """ê¸°ì‚¬ë“¤ì„ Supabase DBì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            # JTBC media_id ì¡°íšŒ (ìµœì´ˆ 1íšŒë§Œ)
            if not hasattr(self, 'media_id'):
                media = await self.article_service.get_or_create_media("JTBC ë‰´ìŠ¤")
                if media:
                    self.media_id = media["id"]
                else:
                    logger.error("JTBC ë‰´ìŠ¤ media_outlets ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨!")
                    return 0
            
            # Dictë¥¼ Article ê°ì²´ë¡œ ë³€í™˜
            article_objects = []
            for article_dict in articles:
                # published_atì„ datetime ê°ì²´ë¡œ ë³€í™˜
                published_at = None
                if article_dict["published_at"]:
                    try:
                        if isinstance(article_dict["published_at"], str):
                            published_at = datetime.fromisoformat(article_dict["published_at"])
                        else:
                            published_at = article_dict["published_at"]
                    except:
                        published_at = datetime.now()
                
                article = Article(
                    title=article_dict["title"],
                    url=article_dict["url"],
                    content_full=article_dict.get("content_full"),
                    published_at=article_dict.get("published_at"),
                    author=article_dict.get("author"),
                    image_url=article_dict.get("image_url"),
                    category=article_dict["category"],
                    media_id=self.media_id,
                    bias=self.bias
                )
                article_objects.append(article)
            
            saved_count = await self.article_service.save_articles(article_objects)
            logger.info(f"ğŸ’¾ {saved_count}ê°œ ê¸°ì‚¬ DB ì €ì¥ ì™„ë£Œ")
            return saved_count
        except Exception as e:
            logger.error(f"DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return 0

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    config = CrawlerConfig()
    crawler = JTBCNewsCrawler(config)
    
    # ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
    articles = await crawler.crawl_all_categories(test_mode=False)
    
    if articles:
        filename = await crawler.save_articles(articles)
        ConsoleUI.print_summary(len(articles), filename)
    else:
        print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main()) 