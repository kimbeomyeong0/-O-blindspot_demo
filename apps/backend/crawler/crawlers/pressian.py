import asyncio
import json
import logging
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Set, Tuple, Any
from dataclasses import dataclass
from enum import Enum

import aiofiles
from bs4 import BeautifulSoup, Tag
from playwright.async_api import async_playwright, Browser, Page

# Supabase ì—°ë™
# sys.path.append(str(Path(__file__).parent.parent))  # ì‚­ì œ
from apps.backend.app.services.article_service import ArticleService
from apps.backend.app.models.article import Article
from apps.backend.crawler.base import BaseNewsCrawler
from apps.backend.crawler.utils import dict_to_article

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn
from rich.theme import Theme
from rich.live import Live
from rich.spinner import Spinner

console = Console(theme=Theme({
    "success": "bold green",
    "fail": "bold red",
    "info": "bold cyan"
}))

logger = logging.getLogger(__name__)

def print_status(msg, status="info"):
    console.print(msg, style=status)

class Category(Enum):
    ECONOMY = "ê²½ì œ"
    # í™•ì¥ì„±: ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ ì¶”ê°€ ê°€ëŠ¥

@dataclass
class CrawlerConfig:
    max_pages: int = 4  # 1~4í˜ì´ì§€ ìˆœíšŒ(30ê°œ ë¯¸ë§Œì‹œ ì¶”ê°€)
    articles_per_category: int = 30
    page_timeout: int = 20000
    wait_timeout: int = 2000
    min_content_length: int = 30
    min_title_length: int = 5

class ConsoleUI:
    @staticmethod
    def print_header():
        console.rule("[bold blue]ğŸš€ í”„ë ˆì‹œì•ˆ í¬ë¡¤ëŸ¬ ì‹œì‘")
    @staticmethod
    def print_category_start(category: str):
        console.print(f"[bold cyan]\nğŸ“° {category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¤‘...[/bold cyan]")
    @staticmethod
    def print_category_complete(category: str, count: int):
        console.print(f"[bold green]âœ… {category}: {count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ[/bold green]")
    @staticmethod
    def print_summary(total_articles: int, filepath: str):
        console.rule("[bold magenta]ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        console.print(f"[bold yellow]ğŸ“Š ì´ {total_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        console.print(f"[bold yellow]ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {filepath}")
        console.rule()

class ArticleExtractor:
    def __init__(self, config: CrawlerConfig):
        self.config = config

    def _parse_img_url(self, style: str) -> Optional[str]:
        # style="background-image:url('/_resources/10/2025/07/14/2025071415353074636_l.s.jpg')"
        # styleì´ listë©´ ì²« ë²ˆì§¸, Noneì´ë©´ ë¹ˆ ë¬¸ìì—´
        if isinstance(style, list):
            style = style[0] if style else ""
        if not isinstance(style, str):
            style = ""
        m = re.search(r"background-image:url\(['\"]?(.*?)['\"]?\)", style)
        if m:
            url = m.group(1)
            if url.startswith("/"):
                return f"https://www.pressian.com{url}"
            return url
        return None

    def _parse_datetime(self, date_str: str) -> Optional[datetime]:
        # robust: í•œê¸€ ì ‘ë‘ì‚¬, ë¶ˆí•„ìš”í•œ ë¬¸ì, ê³µë°± ë“± ì œê±°
        if date_str:
            import re
            date_str = re.sub(r'^(ê¸°ì‚¬ì…ë ¥|ì…ë ¥|ìˆ˜ì •|ë“±ë¡)[ :\-]*', '', date_str)
            date_str = date_str.replace(".", "-").replace("  ", " ").strip()
            date_str = re.sub(r"[^0-9\- :]+", "", date_str)
        for fmt in ["%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M"]:
            try:
                return datetime.strptime(date_str, fmt)
            except Exception:
                continue
        return None

    def parse_article_list(self, html: str) -> List[dict]:
        soup = BeautifulSoup(html, "html.parser")
        articles = []
        # ì¹´ë“œ HTML êµ¬ì¡°ì— ë§ê²Œ ì…€ë ‰í„° ì ê²€
        for li in soup.select('div.section.list_arl_group ul.list > li'):
            try:
                thumb_a = li.select_one('div.thumb a')
                url = None
                if thumb_a:
                    href = thumb_a.get('href')
                    if isinstance(href, list):
                        href = href[0] if href else None
                    if href and isinstance(href, str):
                        url = href if href.startswith('http') else f"https://www.pressian.com{href}"
                if not url:
                    continue
                title_tag = li.select_one('p.title a')
                title = title_tag.get_text(strip=True) if title_tag else None
                if not title:
                    continue
                subtitle_tag = li.select_one('p.sub_title a')
                subtitle = subtitle_tag.get_text(strip=True) if subtitle_tag else None
                summary_tag = li.select_one('p.body a')
                summary = summary_tag.get_text(strip=True) if summary_tag else None
                arl_img = li.select_one('div.arl_img')
                img_style = arl_img['style'] if arl_img and arl_img.has_attr('style') else None
                # img_styleì„ strë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
                if isinstance(img_style, list):
                    img_style = img_style[0] if img_style else ""
                if not isinstance(img_style, str):
                    img_style = ""
                image_url = self._parse_img_url(img_style) if img_style else None
                # robust: bylineê³¼ date ì¶”ì¶œ ê°œì„ 
                byline = li.select_one('div.byline')
                reporter = None
                date = None
                if byline:
                    name_tag = byline.select_one('p.name')
                    date_tag = byline.select_one('p.date')
                    reporter = name_tag.get_text(strip=True) if name_tag else None
                    date = date_tag.get_text(strip=True) if date_tag else None
                    # richë¡œ ì¹´ë“œë³„ ì¶”ì¶œëœ ë‚ ì§œ ë””ë²„ê¹…
                    from rich.console import Console
                    console = Console()
                    console.print(f"[magenta]ì¹´ë“œ ë‚ ì§œ ì¶”ì¶œ: {date} ({title[:30]}...)[/magenta]")
                articles.append({
                    "url": url,
                    "title": title,
                    "subtitle": subtitle,
                    "summary": summary,
                    "image_url": image_url,
                    "author": reporter,
                    "published_at": date
                })
            except Exception as e:
                from rich.console import Console
                console = Console()
                console.print(f"[red]ì¹´ë“œ íŒŒì‹± ì˜¤ë¥˜: {e}[/red]")
                continue
        return articles

    def parse_article_detail(self, html: str) -> dict:
        soup = BeautifulSoup(html, "html.parser")
        # ì œëª©/ë¶€ì œëª©
        title = None
        subtitle = None
        title_tag = soup.select_one('div.view_header p.title')
        if title_tag:
            title = title_tag.get_text(strip=True)
        subtitle_tag = soup.select_one('div.view_header p.sub_title')
        if subtitle_tag:
            subtitle = subtitle_tag.get_text(strip=True)
        # ê¸°ìëª…/ë‚ ì§œ
        byline = soup.select_one('div.byline')
        author = None
        date = None
        if byline:
            name_tag = byline.select_one('span.name')
            date_tag = byline.select_one('span.date')
            author = name_tag.get_text(strip=True) if name_tag else None
            date = date_tag.get_text(strip=True) if date_tag else None
        # ë³¸ë¬¸
        body_div = soup.select_one('div.article_body')
        body = ""
        if body_div:
            for tag in body_div.find_all(['p', 'div', 'figure']):
                if tag.name == 'figure':
                    img = tag.find('img')
                    # Tag íƒ€ì…ì¸ì§€ í™•ì¸ í›„ get('src')
                    if img and hasattr(img, 'get'):
                        src = img.get('src')
                        if src:
                            body += f"<img src='{src}' />\n"
                    continue
                text = tag.get_text(strip=True)
                if text:
                    body += text + "\n"
        # ë³¸ë¬¸ ì´ë¯¸ì§€(ì²«ë²ˆì§¸)
        image_url = None
        if body_div:
            img_tag = body_div.find('img')
            # Tag íƒ€ì…ì¸ì§€ ëª…í™•íˆ ì²´í¬
            if img_tag and isinstance(img_tag, Tag):
                src = img_tag.get('src')
                if src:
                    image_url = src
        return {
            "title": title,
            "subtitle": subtitle,
            "author": author,
            "published_at": date,
            "content_full": body.strip(),
            "image_url": image_url
        }

class PressianCrawler(BaseNewsCrawler):
    CATEGORY_URLS = {
        Category.ECONOMY: [
            "https://www.pressian.com/pages/news-economy-list",
            "https://www.pressian.com/pages/news-economy-list?page=2",
            "https://www.pressian.com/pages/news-economy-list?page=3",
            "https://www.pressian.com/pages/news-economy-list?page=4"
        ]
    }

    def __init__(self, config: CrawlerConfig):
        self.config = config
        self.extractor = ArticleExtractor(config)
        self.article_service = ArticleService()
        self.ui = ConsoleUI()

    async def crawl_category(self, browser: Browser, category: Category) -> List[Dict[str, Any]]:
        articles = []
        seen_urls = set()
        for page_url in self.CATEGORY_URLS[category]:
            page = await browser.new_page()
            try:
                await page.goto(page_url, wait_until="domcontentloaded", timeout=self.config.page_timeout)
                await page.wait_for_timeout(self.config.wait_timeout)
                html = await page.content()
                page_articles = self.extractor.parse_article_list(html)
                for art in page_articles:
                    if art['url'] not in seen_urls:
                        seen_urls.add(art['url'])
                        art['category'] = category.value  # ì¹´í…Œê³ ë¦¬ í•„ë“œ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€
                        articles.append(art)
                if len(articles) >= self.config.articles_per_category:
                    articles = articles[:self.config.articles_per_category]
                    break
            finally:
                await page.close()
        return articles

    async def enrich_and_parse_details(self, browser: Browser, articles: List[dict]) -> List[dict]:
        enriched = []
        semaphore = asyncio.Semaphore(5)  # ë™ì‹œì— 5ê°œê¹Œì§€
        async def parse_one(art, idx):
            async with semaphore:
                page = await browser.new_page()
                try:
                    await page.goto(art['url'], wait_until="domcontentloaded", timeout=self.config.page_timeout)
                    await page.wait_for_timeout(self.config.wait_timeout)
                    html = await page.content()
                    detail = self.extractor.parse_article_detail(html)
                    # ì¹´ë“œì—ì„œ ë°›ì€ published_at ë°±ì—… ì‚¬ìš©
                    card_published_at = art.get('published_at')
                    detail_published_at = detail.get('published_at')
                    from rich.console import Console
                    console = Console()
                    console.print(f"[yellow]ìƒì„¸ íŒŒì‹±: ì¹´ë“œ={card_published_at}, ìƒì„¸={detail_published_at} ({art['title'][:30]}...)[/yellow]")
                    # ìƒì„¸ ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜ ì‹œë„
                    if detail_published_at:
                        detail_published_at_dt = self.extractor._parse_datetime(detail_published_at)
                        if detail_published_at_dt:
                            detail['published_at'] = detail_published_at_dt
                            console.print(f"[green]ìƒì„¸ íŒŒì‹±: ìƒì„¸ ë‚ ì§œ ë³€í™˜ ì„±ê³µ - {detail_published_at_dt} ({art['title'][:30]}...)[/green]")
                        else:
                            # ìƒì„¸ ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì¹´ë“œ ë‚ ì§œ ì‚¬ìš©
                            if card_published_at:
                                card_published_at_dt = self.extractor._parse_datetime(card_published_at)
                                if card_published_at_dt:
                                    detail['published_at'] = card_published_at_dt
                                    console.print(f"[cyan]ìƒì„¸ íŒŒì‹±: ì¹´ë“œ ë‚ ì§œ ë°±ì—… ì‚¬ìš© - {card_published_at_dt} ({art['title'][:30]}...)[/cyan]")
                                else:
                                    detail['published_at'] = None
                                    console.print(f"[red]ìƒì„¸ íŒŒì‹±: ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ - {art['title'][:30]}...)[/red]")
                            else:
                                detail['published_at'] = None
                                console.print(f"[red]ìƒì„¸ íŒŒì‹±: ë‚ ì§œ ì—†ìŒ - {art['title'][:30]}...)[/red]")
                    elif card_published_at:
                        # ìƒì„¸ì— ë‚ ì§œê°€ ì—†ìœ¼ë©´ ì¹´ë“œ ë‚ ì§œ ì‚¬ìš©
                        card_published_at_dt = self.extractor._parse_datetime(card_published_at)
                        if card_published_at_dt:
                            detail['published_at'] = card_published_at_dt
                            console.print(f"[cyan]ìƒì„¸ íŒŒì‹±: ì¹´ë“œ ë‚ ì§œ ë°±ì—… ì‚¬ìš© - {card_published_at_dt} ({art['title'][:30]}...)[/cyan]")
                        else:
                            detail['published_at'] = None
                            console.print(f"[red]ìƒì„¸ íŒŒì‹±: ì¹´ë“œ ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ - {art['title'][:30]}...)[/red]")
                    else:
                        detail['published_at'] = None
                        console.print(f"[red]ìƒì„¸ íŒŒì‹±: ë‚ ì§œ ì—†ìŒ - {art['title'][:30]}...)[/red]")
                    merged = {**art, **{k: v for k, v in detail.items() if v}}
                    print_status(f"âœ” {art['title'][:40]} ... ì„±ê³µ", "success")
                    return merged
                except Exception as e:
                    print_status(f"âœ– {art['title'][:40]} ... ì˜¤ë¥˜: {e}", "fail")
                    return None
                finally:
                    await page.close()
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            "[progress.percentage]{task.percentage:>3.0f}%",
            TimeElapsedColumn(),
            console=console,
        ) as progress:
            task = progress.add_task(f"ìƒì„¸ ê¸°ì‚¬ íŒŒì‹± ì¤‘...", total=len(articles))
            tasks = [parse_one(art, i) for i, art in enumerate(articles, 1)]
            for f in asyncio.as_completed(tasks):
                result = await f
                if result:
                    enriched.append(result)
                progress.update(task, advance=1)
        return enriched

    async def crawl_all_categories(self, test_mode: bool = False) -> List[Dict[str, Any]]:
        self.ui.print_header()
        all_articles = []
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            try:
                for category in Category:
                    self.ui.print_category_start(category.value)
                    articles = await self.crawl_category(browser, category)
                    if not articles:
                        continue
                    articles = await self.enrich_and_parse_details(browser, articles)
                    self.ui.print_category_complete(category.value, len(articles))
                    all_articles.extend(articles)
                    if test_mode:
                        break
            finally:
                await browser.close()
        return all_articles

    async def save_articles(self, articles: List[Dict[str, Any]]) -> str:
        if not articles:
            logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return ""
        data_dir = Path("data/raw")
        data_dir.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"pressian_articles_{timestamp}.jsonl"
        filepath = data_dir / filename
        async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
            for article in articles:
                # datetime ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
                article_copy = article.copy()
                if article_copy.get('published_at') and isinstance(article_copy['published_at'], datetime):
                    article_copy['published_at'] = article_copy['published_at'].isoformat()
                await f.write(json.dumps(article_copy, ensure_ascii=False) + '\n')
        return str(filepath)

    async def save_articles_to_db(self, articles: List[Dict[str, Any]]) -> int:
        if not articles:
            logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        # media_id, bias ìë™ ì¡°íšŒ/í• ë‹¹
        for art in articles:
            media_info = await self.article_service.get_or_create_media("í”„ë ˆì‹œì•ˆ")
            art['media_id'] = media_info['id'] if media_info else None
            art['bias'] = media_info['bias'] if media_info else 'center'
            # published_at datetime robust ë³€í™˜ (ì´ë¯¸ datetimeì´ë©´ ë³€í™˜í•˜ì§€ ì•ŠìŒ)
            if art.get('published_at'):
                if isinstance(art['published_at'], datetime):
                    # ì´ë¯¸ datetime ê°ì²´ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    pass
                elif isinstance(art['published_at'], str):
                    # ë¬¸ìì—´ì¸ ê²½ìš°ì—ë§Œ íŒŒì‹± ì‹œë„
                    try:
                        parsed_dt = self.extractor._parse_datetime(art['published_at'])
                        if parsed_dt:
                            art['published_at'] = parsed_dt
                        else:
                            print_status(f"[ê²½ê³ ] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {art.get('title')} / ì›ë³¸: {art.get('published_at')}", "fail")
                            art['published_at'] = None
                    except Exception:
                        print_status(f"[ê²½ê³ ] ë‚ ì§œ íŒŒì‹± ì˜ˆì™¸: {art.get('title')}", "fail")
                        art['published_at'] = None
                else:
                    # ë‹¤ë¥¸ íƒ€ì…ì¸ ê²½ìš° Noneìœ¼ë¡œ ì„¤ì •
                    art['published_at'] = None
        # Article ëª¨ë¸ë¡œ ë³€í™˜
        article_objects = [dict_to_article(art) for art in articles]
        saved_count = await self.article_service.save_articles(article_objects)
        return saved_count

# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜ˆì‹œ(main)
async def main():
    config = CrawlerConfig()
    crawler = PressianCrawler(config)
    articles = await crawler.crawl_all_categories()
    if articles:
        filepath = await crawler.save_articles(articles)
        saved_count = await crawler.save_articles_to_db(articles)
        ConsoleUI.print_summary(len(articles), filepath)
        print_status(f"DB ì €ì¥: {saved_count}ê±´", "success")
    else:
        print_status("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.", "fail")

if __name__ == "__main__":
    asyncio.run(main()) 