import asyncio
import json
import logging
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Set, Tuple, Any
from dataclasses import dataclass
from enum import Enum

import aiofiles
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Browser, Page

# Supabase ì—°ë™ì„ ìœ„í•œ import
import sys
sys.path.append(str(Path(__file__).parent.parent))
from apps.backend.app.services.article_service import ArticleService
from apps.backend.app.models.article import Article
from apps.backend.crawler.base import BaseNewsCrawler

from rich.console import Console
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn
from rich.theme import Theme
from rich.live import Live
from rich.spinner import Spinner

console = Console(theme=Theme({
    "success": "bold green",
    "fail": "bold red",
    "info": "bold cyan"
}))

def print_status(msg, status="info"):
    console.print(msg, style=status)

# ë¡œê¹… ì„¤ì • - íŒŒì¼ê³¼ ì½˜ì†” ë¶„ë¦¬
def setup_logging():
    """ë¡œê¹… ì„¤ì •ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # íŒŒì¼ ë¡œê±° (ìƒì„¸ ì •ë³´)
    file_handler = logging.FileHandler(log_dir / "crawler_detailed.log", encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)
    file_formatter = logging.Formatter(
        "%(asctime)s %(name)s %(levelname)s %(message)s"
    )
    file_handler.setFormatter(file_formatter)
    
    # ì½˜ì†” ë¡œê±° (ê°„ë‹¨í•œ ì •ë³´ë§Œ)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter("%(message)s")
    console_handler.setFormatter(console_formatter)
    
    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    logging.basicConfig(
        level=logging.DEBUG,
        handlers=[file_handler, console_handler]
    )

setup_logging()
logger = logging.getLogger(__name__)

# ì™¸ë¶€ noisy ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê¹… ë ˆë²¨ ìµœì†Œí™”
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("supabase").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("requests").setLevel(logging.WARNING)

class Category(Enum):
    """í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ ì •ì˜"""
    ECONOMY = "ê²½ì œ"

@dataclass
class CrawlerConfig:
    """í¬ë¡¤ëŸ¬ ì„¤ì • í´ë˜ìŠ¤"""
    max_pages: int = 2  # 30ê°œ ê¸°ì‚¬ë¥¼ ìœ„í•´ 2í˜ì´ì§€ (1í˜ì´ì§€ë‹¹ 20ê°œ)
    articles_per_category: int = 30
    page_timeout: int = 20000
    wait_timeout: int = 2000
    min_content_length: int = 5  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì™„í™”
    min_title_length: int = 10
    
    # CSS ì…€ë ‰í„°ë“¤ - ì˜¤ë§ˆì´ë‰´ìŠ¤ì— ë§ê²Œ ë³´ê°•
    headline_selector: str = '.list_article a'
    title_selectors: Optional[List[str]] = None
    content_selectors: Optional[List[str]] = None
    date_selectors: Optional[List[str]] = None
    author_selectors: Optional[List[str]] = None
    image_selectors: Optional[List[str]] = None
    
    def __post_init__(self):
        if self.title_selectors is None:
            self.title_selectors = ['h1', '.article-title', '.story-title', '.title']
        if self.content_selectors is None:
            self.content_selectors = [
                'div#article_view', 'div.article_view', '.article_view', '#article_view',
                '.content', '.article_content', '.news_body', '.view_content', '.view',
                '.article-body', '.news-article-body', '.story-content', '.entry-content',
                '#article-body', '.article-content', '.text-content', '.content-body',
                '.article-text', '.story-body'
            ]
        if self.date_selectors is None:
            self.date_selectors = [
                "meta[property='article:published_time']", 
                "meta[property='og:article:published_time']", 
                "time", ".published-date", ".article-date", ".date"
            ]
        if self.author_selectors is None:
            self.author_selectors = [
                "meta[property='og:article:author']", 
                ".author", ".byline", ".writer", ".reporter", ".writer_name"
            ]
        if self.image_selectors is None:
            self.image_selectors = [
                "meta[property='og:image']", 
                "meta[property='twitter:image']", 
                ".article-image img", ".story-image img", ".article_img img"
            ]

# ê¸°ì¡´ ConsoleUI í´ë˜ìŠ¤ëŠ” rich ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì²´
class ConsoleUI:
    @staticmethod
    def print_header():
        console.rule("[bold blue]ğŸš€ ì˜¤ë§ˆì´ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì‹œì‘")
    @staticmethod
    def print_category_start(category: str):
        console.print(f"[bold cyan]\nğŸ“° {category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¤‘...[/bold cyan]")
    @staticmethod
    def print_category_complete(category: str, count: int):
        console.print(f"[bold green]âœ… {category}: {count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ[/bold green]")
    @staticmethod
    def print_summary(total_articles: int, filepath: str):
        console.rule("[bold magenta]ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        console.print(f"[bold yellow]ğŸ“Š ì´ {total_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        console.print(f"[bold yellow]ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {filepath}")
        console.rule()

class ArticleExtractor:
    """ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ í´ë˜ìŠ¤"""
    
    def __init__(self, config: CrawlerConfig):
        self.config = config
    
    async def extract_article_content(self, page: Page, url: str) -> Optional[Dict[str, Any]]:
        """ê¸°ì‚¬ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=self.config.page_timeout)
            # ë³¸ë¬¸ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            try:
                await page.wait_for_selector('.at_contents', timeout=5000)
            except Exception:
                pass  # ì…€ë ‰í„°ê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ì§„í–‰
            await page.wait_for_timeout(2000)
            
            html = await page.content()
            # ë””ë²„ê¹…ìš© HTML ì €ì¥
            with open("debug_ohmynews.html", "w", encoding="utf-8") as f:
                f.write(html)
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = self._extract_title(soup)
            if not title:
                logger.debug(f"ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}")
                return None
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = self._extract_content(soup)
            if not content or len(content.strip()) < self.config.min_content_length:
                logger.warning(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {url} | ì œëª©: {title}")
                return None
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            published_at = self._extract_published_date(soup)
            author = self._extract_author(soup)
            image_url = self._extract_image_url(soup)
            
            return {
                "title": title,
                "url": url,
                "content_full": content,
                "published_at": published_at or datetime.now().isoformat(),
                "author": author,
                "image_url": image_url
            }
            
        except Exception as e:
            logger.debug(f"ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {str(e)}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """ì˜¤ë§ˆì´ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        # ì˜¤ë§ˆì´ë‰´ìŠ¤ ì œëª© ì…€ë ‰í„°ë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
        title_selectors = [
            'h2.article_tit a',  # ì˜¤ë§ˆì´ë‰´ìŠ¤ ë©”ì¸ ì œëª©
            'h2.article_tit',    # ì œëª© ì˜ì—­
            'h1.article_tit a',  # h1 ì œëª©
            'h1.article_tit',    # h1 ì œëª© ì˜ì—­
            '.article_tit a',     # ì œëª© ë§í¬
            '.article_tit',       # ì œëª© ì˜ì—­
            'h1',                 # ì¼ë°˜ h1
            'h2',                 # ì¼ë°˜ h2
            '.title',             # ì¼ë°˜ ì œëª©
        ]
        
        for selector in title_selectors:
            el = soup.select_one(selector)
            if el:
                title = el.get_text(strip=True)
                if title and len(title) >= self.config.min_title_length:
                    return title
        
        # ê¸°ì¡´ title_selectorsë„ fallbackìœ¼ë¡œ ì‚¬ìš©
        if self.config.title_selectors is not None:
            for selector in self.config.title_selectors:
                el = soup.select_one(selector)
                if el:
                    title = el.get_text(strip=True)
                    if title and len(title) >= self.config.min_title_length:
                        return title
        
        return None
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """ì˜¤ë§ˆì´ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        content_parts = []
        
        # .at_contentsê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš© (ì˜¤ë§ˆì´ë‰´ìŠ¤ ë³¸ë¬¸ ì˜ì—­)
        at_contents = soup.select_one('.at_contents')
        if at_contents:
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ë“¤ ì œê±°
            unwanted_selectors = [
                'figure.omn-photo',  # ì´ë¯¸ì§€
                'script',            # ìŠ¤í¬ë¦½íŠ¸
                'style',             # ìŠ¤íƒ€ì¼
                'iframe',            # iframe
                'div[id^="dv"]',     # ê´‘ê³  div (dvë¡œ ì‹œì‘í•˜ëŠ” id)
                'div[id^="google_ads"]',  # êµ¬ê¸€ ê´‘ê³ 
                'div[class*="ad"]',  # ê´‘ê³  í´ë˜ìŠ¤
                'div[class*="advertisement"]',  # ê´‘ê³  í´ë˜ìŠ¤
                'div[class*="V0"]',  # ê´‘ê³  í´ë˜ìŠ¤
                'div[class*="ohmynews_article"]',  # ê´‘ê³  í´ë˜ìŠ¤
                'div[id*="CenterAd"]',  # ì¤‘ì•™ ê´‘ê³ 
                'div[class*="livere"]',  # ëŒ“ê¸€ ì‹œìŠ¤í…œ
                'div[class*="dable"]',   # Dable ìœ„ì ¯
                'div[class*="gallery"]', # ê°¤ëŸ¬ë¦¬
                'div[class*="reporter"]', # ê¸°ì ì •ë³´
                'div[class*="copyright"]', # ì €ì‘ê¶Œ
                'div[class*="tag_area"]', # íƒœê·¸ ì˜ì—­
                'div[class*="arc-bottom"]', # í•˜ë‹¨ ì˜ì—­
                'div[class*="support-box"]', # ì§€ì› ë°•ìŠ¤
                'div[class*="layer"]', # ë ˆì´ì–´
                'div[class*="ad_area"]', # ê´‘ê³  ì˜ì—­
            ]
            
            for selector in unwanted_selectors:
                for unwanted in at_contents.select(selector):
                    unwanted.decompose()
            
            # <br> íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜
            for br in at_contents.find_all("br"):
                br.replace_with("\n")
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = at_contents.get_text(separator="\n", strip=True)
            
            # ë¹ˆ ì¤„ ì •ë¦¬ ë° í…ìŠ¤íŠ¸ ì •ë¦¬
            lines = []
            for line in text.splitlines():
                line = line.strip()
                if line and len(line) > 1:  # ë¹ˆ ì¤„ì´ ì•„ë‹ˆê³  1ê¸€ìë³´ë‹¤ ê¸´ ê²½ìš°ë§Œ
                    lines.append(line)
            
            if lines:
                content_parts.append("\n".join(lines))
        
        # .at_contentsê°€ ì—†ê±°ë‚˜ ë‚´ìš©ì´ ë¶€ì¡±í•œ ê²½ìš° ê¸°ì¡´ content_selectors ì‚¬ìš©
        if not content_parts and self.config.content_selectors is not None:
            for selector in self.config.content_selectors:
                elements = soup.select(selector)
                for element in elements:
                    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                    for unwanted in element.find_all(['script', 'style', 'iframe', 'figure']):
                        unwanted.decompose()
                    
                    text = element.get_text(separator="\n", strip=True)
                    if text and len(text) > 10:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                        content_parts.append(text)
        
        return '\n\n'.join(content_parts) if content_parts else ""
    
    def _extract_published_date(self, soup: BeautifulSoup) -> Optional[str]:
        """ì˜¤ë§ˆì´ë‰´ìŠ¤ ê¸°ì‚¬ ë°œí–‰ì¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        # ì˜¤ë§ˆì´ë‰´ìŠ¤ ë‚ ì§œ ì…€ë ‰í„°ë“¤
        date_selectors = [
            '.atc-sponsor .date',  # ì˜¤ë§ˆì´ë‰´ìŠ¤ ë‚ ì§œ
            '.atc-sponsor span.date', # ë‚ ì§œ span
            '.info-list .date',    # ì •ë³´ ì˜ì—­ ë‚ ì§œ
            '.article-info .date',  # ê¸°ì‚¬ ì •ë³´ ë‚ ì§œ
            '.published-date',      # ë°œí–‰ì¼
            '.article-date',        # ê¸°ì‚¬ ë‚ ì§œ
            '.date',               # ì¼ë°˜ ë‚ ì§œ
        ]
        
        for selector in date_selectors:
            el = soup.select_one(selector)
            if el:
                date_str = el.get_text(strip=True)
                if date_str:
                    # ë‚ ì§œ í˜•ì‹ ì •ë¦¬ (ì˜ˆ: "25.07.14 15:48" -> "2025-07-14T15:48:00")
                    date_str = re.sub(r'[^\d\.\-\s:]', '', str(date_str)).strip()
                    if date_str:
                        # ì˜¤ë§ˆì´ë‰´ìŠ¤ ë‚ ì§œ í˜•ì‹ ë³€í™˜ (25.07.14 -> 2025-07-14)
                        if re.match(r'\d{2}\.\d{2}\.\d{2}', date_str):
                            parts = date_str.split('.')
                            if len(parts) >= 3:
                                year = f"20{parts[0]}"
                                month = parts[1]
                                day = parts[2]
                                date_str = f"{year}-{month}-{day}"
                        return date_str
        
        # ê¸°ì¡´ date_selectorsë„ fallbackìœ¼ë¡œ ì‚¬ìš©
        if self.config.date_selectors is not None:
            for selector in self.config.date_selectors:
                el = soup.select_one(selector)
                if el:
                    if selector.startswith('meta'):
                        date_str = el.get('content', '')
                    else:
                        date_str = el.get_text(strip=True)
                    
                    if date_str:
                        # ë‚ ì§œ í˜•ì‹ ì •ë¦¬
                        date_str = re.sub(r'[^\d\-\s:]', '', str(date_str)).strip()
                        if date_str:
                            return date_str
        
        return None
    
    def _extract_author(self, soup: BeautifulSoup) -> Optional[str]:
        """ì˜¤ë§ˆì´ë‰´ìŠ¤ ê¸°ì‚¬ ì‘ì„±ìë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        # ì˜¤ë§ˆì´ë‰´ìŠ¤ ì‘ì„±ì ì…€ë ‰í„°ë“¤
        author_selectors = [
            '.info-list strong',  # ê¸°ìëª… (strong íƒœê·¸)
            '.info-list a strong', # ê¸°ìëª… ë§í¬ ë‚´ë¶€
            '.lk_my strong',       # ê¸°ì ë§í¬
            '.reporter strong',    # ê¸°ì ì •ë³´
            '.writer strong',      # ì‘ì„±ì
            '.author strong',      # ì €ì
            '.byline strong',      # ë°”ì´ë¼ì¸
            '.info-list .tt01',    # ê¸°ì ID
            '.lk_my .tt01',        # ê¸°ì ID
            '.reporter .tt01',     # ê¸°ì ID
        ]
        
        for selector in author_selectors:
            el = soup.select_one(selector)
            if el:
                author = el.get_text(strip=True)
                if author and len(author) > 1:
                    return author
        
        # ê¸°ì¡´ author_selectorsë„ fallbackìœ¼ë¡œ ì‚¬ìš©
        if self.config.author_selectors is not None:
            for selector in self.config.author_selectors:
                el = soup.select_one(selector)
                if el:
                    if selector.startswith('meta'):
                        author = el.get('content', '')
                    else:
                        author = el.get_text(strip=True)
                    
                    if author and len(str(author)) > 1:
                        return str(author)
        
        return None
    
    def _extract_image_url(self, soup: BeautifulSoup) -> Optional[str]:
        """ì´ë¯¸ì§€ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if self.config.image_selectors is None:
            return None
            
        for selector in self.config.image_selectors:
            el = soup.select_one(selector)
            if el:
                if selector.startswith('meta'):
                    image_url = el.get('content', '')
                else:
                    image_url = el.get('src', '')
                
                if image_url and str(image_url).startswith('http'):
                    return str(image_url)
        return None

# ê¸°ì¡´ ì½”ë“œ(ArticleExtractor, CrawlerConfig ë“±)ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€

# ê¸°ì‚¬ë³„ ì²˜ë¦¬ì— rich Progress ì ìš©
from contextlib import contextmanager

@contextmanager
def rich_spinner(msg: str):
    with Live(Spinner("dots", text=msg), refresh_per_second=10, console=console):
        yield

class OhmynewsCrawler(BaseNewsCrawler):
    """ì˜¤ë§ˆì´ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""
    
    CATEGORY_URLS = {
        Category.ECONOMY: "https://www.ohmynews.com/NWS_Web/ArticlePage/Total_Article.aspx?PAGE_CD=C0300"
    }
    
    def __init__(self, config: CrawlerConfig):
        self.config = config
        self.extractor = ArticleExtractor(config)
        self.article_service = ArticleService()
    
    async def crawl_category(self, browser: Browser, category: Category) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        articles = []
        url = self.CATEGORY_URLS[category]
        
        page = await browser.new_page()
        try:
            for page_num in range(1, self.config.max_pages + 1):
                if page_num == 1:
                    page_url = url
                else:
                    page_url = f"{url}&pageno={page_num}"
                # [DEBUG] í˜ì´ì§€ ì´ë™ ë¡œê·¸ (URLë§Œ ë‚¨ê¸°ê³  ê¸°ì‚¬ URL ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ì€ ì œê±°)
                await page.goto(page_url, wait_until="domcontentloaded", timeout=self.config.page_timeout)
                await page.wait_for_timeout(self.config.wait_timeout)
                
                # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
                links_and_titles = await self._extract_article_links(page, category)
                # (ê¸°ì‚¬ URL ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ ì œê±°)
                
                if not links_and_titles:
                    logger.warning(f"í˜ì´ì§€ {page_num}ì—ì„œ ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                # ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ
                page_articles = await self._extract_articles(page, category, links_and_titles)
                articles.extend(page_articles)
                
                # ëª©í‘œ ê¸°ì‚¬ ìˆ˜ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
                if len(articles) >= self.config.articles_per_category:
                    articles = articles[:self.config.articles_per_category]
                    break
                
                logger.info(f"í˜ì´ì§€ {page_num}: {len(page_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        
        finally:
            await page.close()
        
        return articles
    
    async def _extract_article_links(self, page: Page, category: Category) -> List[Tuple[str, str]]:
        """ê¸°ì‚¬ ë§í¬ì™€ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            # ìƒë‹¨ ì£¼ìš” ê¸°ì‚¬ì™€ ì¼ë°˜ ê¸°ì‚¬ ëª¨ë‘ í¬í•¨
            article_elements = await page.query_selector_all(
                'div.top_news a, ul.list_type1 dt > a'
            )
            
            links_and_titles = []
            for element in article_elements:
                try:
                    href = await element.get_attribute('href')
                    title = await element.text_content()
                    # ì‹¤ì œ ê¸°ì‚¬ ë³¸ë¬¸ ë§í¬ë§Œ ì¶”ì¶œ
                    if href and '/NWS_Web/View/at_pg.aspx?CNTN_CD=' in href:
                        if href.startswith('/'):
                            href = f"https://www.ohmynews.com{href}"
                        if href and title:
                            links_and_titles.append((href.strip(), title.strip()))
                except Exception as e:
                    logger.debug(f"ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    continue
            
            return links_and_titles
            
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    async def _extract_articles(self, page: Page, category: Category, links_and_titles: List[Tuple[str, str]]) -> List[Dict[str, Any]]:
        articles = []
        total = len(links_and_titles)
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            "[progress.percentage]{task.percentage:>3.0f}%",
            TimeElapsedColumn(),
            console=console,
        ) as progress:
            task = progress.add_task(f"ê¸°ì‚¬ ì¶”ì¶œ ì¤‘...", total=total)
            for i, (url, title) in enumerate(links_and_titles, 1):
                progress.update(task, advance=1, description=f"({i}/{total}) {title[:30]}")
                try:
                    article_data = await self.extractor.extract_article_content(page, url)
                    if article_data:
                        article_data['category'] = category.value
                        article_data['source'] = 'ì˜¤ë§ˆì´ë‰´ìŠ¤'
                        articles.append(article_data)
                        print_status(f"âœ” {title[:40]} ... ì„±ê³µ", "success")
                    else:
                        print_status(f"âœ– {title[:40]} ... ë³¸ë¬¸ ì—†ìŒ", "fail")
                    await page.wait_for_timeout(100)
                except Exception as e:
                    print_status(f"âœ– {title[:40]} ... ì˜¤ë¥˜: {e}", "fail")
                    continue
        return articles
    
    async def crawl_all_categories(self, test_mode: bool = False) -> List[Dict[str, Any]]:
        ConsoleUI.print_header()
        
        all_articles = []
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            
            try:
                for category in Category:
                    ConsoleUI.print_category_start(category.value)
                    articles = await self.crawl_category(browser, category)
                    # (ì„ì‹œ ì œí•œ í•´ì œ) ì „ì²´ ê¸°ì‚¬ ì‚¬ìš©
                    all_articles.extend(articles)
                    ConsoleUI.print_category_complete(category.value, len(articles))
                    
                    if test_mode:
                        break
            
            finally:
                await browser.close()
        
        return all_articles
    
    async def save_articles(self, articles: List[Dict[str, Any]]) -> str:
        """ê¸°ì‚¬ë¥¼ JSONL íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        if not articles:
            logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return ""
        
        # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
        data_dir = Path("data/raw")
        data_dir.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ohmynews_articles_{timestamp}.jsonl"
        filepath = data_dir / filename
        
        # JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥
        async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
            for article in articles:
                await f.write(json.dumps(article, ensure_ascii=False) + '\n')
        
        return str(filepath)
    
    async def save_articles_to_db(self, articles: List[Dict[str, Any]]) -> int:
        """ê¸°ì‚¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
        if not articles:
            logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        try:
            # Article ëª¨ë¸ë¡œ ë³€í™˜
            article_objects = []
            for article_data in articles:
                try:
                    bias = article_data.get('bias', '') or 'left'
                    article = Article(
                        title=article_data.get('title', ''),
                        url=article_data.get('url', ''),
                        category=article_data.get('category', ''),
                        content_full=article_data.get('content_full', ''),
                        published_at=None,  # ë¬¸ìì—´ ëŒ€ì‹  Noneìœ¼ë¡œ ì„¤ì •
                        author=article_data.get('author', ''),
                        image_url=article_data.get('image_url', ''),
                        media_id=article_data.get('media_id', '') or '',
                        bias=bias
                    )
                    article_objects.append(article)
                except Exception as e:
                    continue
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            saved_count = await self.article_service.save_articles(article_objects)
            return saved_count
        except Exception as e:
            return 0

# --- ê³µí†µ ìœ í‹¸: ì–¸ë¡ ì‚¬ëª…ìœ¼ë¡œ media_id ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸° ---
import requests, os

def get_media_info(media_name: str):
    # ì˜¤ë§ˆì´ë‰´ìŠ¤ ë“± ì£¼ìš” ì–¸ë¡ ì‚¬ ê¸°ë³¸ê°’ ì‚¬ì „
    default_map = {
        'ì˜¤ë§ˆì´ë‰´ìŠ¤': {
            'media_id': '149dab80-d623-49d7-a0f2-4c52329d2626',  # DBì™€ ì¼ì¹˜í•˜ë„ë¡ ìˆ˜ì •
            'bias': 'left',
        },
        # í•„ìš”ì‹œ ë‹¤ë¥¸ ì–¸ë¡ ì‚¬ë„ ì¶”ê°€
    }
    SUPABASE_URL = os.getenv("SUPABASE_URL")
    SUPABASE_KEY = os.getenv("SUPABASE_KEY")
    if SUPABASE_URL and SUPABASE_KEY:
        headers = {
            "apikey": SUPABASE_KEY,
            "Authorization": f"Bearer {SUPABASE_KEY}"
        }
        url = f"{SUPABASE_URL}/rest/v1/media_outlets?name=eq.{media_name}"
        try:
            resp = requests.get(url, headers=headers, timeout=5)
            if resp.status_code == 200 and resp.json():
                media = resp.json()[0]
                media_id = media.get("id") or default_map.get(media_name, {}).get("media_id", "")
                bias = media.get("bias") or default_map.get(media_name, {}).get("bias", "")
                return media_id, bias
        except Exception as e:
            pass
    d = default_map.get(media_name, {})
    return d.get("media_id", ""), d.get("bias", "")

# --- main í•¨ìˆ˜ì—ì„œ ê³µí†µ ìœ í‹¸ ì‚¬ìš© ---
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    config = CrawlerConfig()
    crawler = OhmynewsCrawler(config)

    # ê³µí†µ ìœ í‹¸ë¡œ media_id ì¡°íšŒ
    media_id, bias = get_media_info('ì˜¤ë§ˆì´ë‰´ìŠ¤')
    # print(f"[DEBUG] ìµœì¢… ì‚¬ìš© media_id: {media_id}")  # ë¶ˆí•„ìš”í•œ ë””ë²„ê·¸ ì¶œë ¥ ì œê±°

    # í¬ë¡¤ë§ ì‹¤í–‰
    articles = await crawler.crawl_all_categories()

    if articles:
        # ê° ê¸°ì‚¬ì— media_id, bias í• ë‹¹ (ì—†ìœ¼ë©´ ì•ˆì „í•œ ê¸°ë³¸ê°’)
        for article in articles:
            article["media_id"] = media_id or '149dab80-d623-49d7-a0f2-4c52329d2626'
            article["bias"] = bias or 'left'
        # íŒŒì¼ë¡œ ì €ì¥
        filepath = await crawler.save_articles(articles)
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        await crawler.save_articles_to_db(articles)
        # ê²°ê³¼ ì¶œë ¥
        ConsoleUI.print_summary(len(articles), filepath)
    else:
        print("í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
