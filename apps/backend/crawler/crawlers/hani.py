import asyncio
from pathlib import Path
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from enum import Enum
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Browser, Page
import os
from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn
from rich.console import Console

# Supabase ì—°ë™ ë° ì„œë¹„ìŠ¤/ëª¨ë¸ import
import sys
sys.path.append(str(Path(__file__).parent.parent))
from apps.backend.app.services.article_service import ArticleService
from apps.backend.app.models.article import Article
from apps.backend.crawler.base import BaseNewsCrawler

class HaniCategory(Enum):
    ECONOMY = "ê²½ì œ"
    # ì¶”í›„ í™•ì¥ ê°€ëŠ¥

@dataclass
class HaniCrawlerConfig:
    articles_per_category: int = 30
    page_timeout: int = 20000
    wait_timeout: int = 2000
    min_content_length: int = 50
    min_title_length: int = 10
    # ì£¼ìš” ì…€ë ‰í„°
    list_selector: str = '#content > div.section_inner__Gn71W > div.section_flexInner__jGNGY.section_content__CNIbB > div.section_left__5BOCT'
    article_selector: str = '#renewal2023'
    title_selector: str = '#renewal2023 > h3'
    date_selector: str = '#renewal2023 > div.ArticleDetailView_articleDetail__IT2fh > ul'
    author_selector: str = '#content > div:nth-child(1) > div > div:nth-child(3) > div.ArticleDetail_reporterWrap__GHM9e > div > div > a > div.ArticleDetailReporter_nameInfo__SPYjX > div.ArticleDetailReporter_name__kXCEK'
    image_selector: str = '#content > div.section_inner__Gn71W > div.section_flexInner__jGNGY.section_content__CNIbB > div.section_left__5BOCT > div > ul > li:nth-child(3) > article > div > a > div > div > div > img'

class HaniCrawler(BaseNewsCrawler):
    CATEGORY_URLS = {
        HaniCategory.ECONOMY: "https://www.hani.co.kr/arti/economy?page={page}"
    }

    def __init__(self, config: HaniCrawlerConfig):
        self.config = config
        self.article_service = ArticleService()
        # ê¸°íƒ€ í•„ìš”í•œ ì´ˆê¸°í™”

    async def fetch_article_list(self, browser: Browser, category: HaniCategory, min_count: int = 30, max_pages: int = 10) -> List[str]:
        """ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ URL ì¶”ì¶œ (ì¤‘ë³µ/íŒŒì‹± ì‹¤íŒ¨ ê³ ë ¤, min_countë§Œí¼ í™•ë³´ë  ë•Œê¹Œì§€ ì—¬ëŸ¬ í˜ì´ì§€ ë°˜ë³µ)"""
        article_urls = set()
        context = await browser.new_context()
        page = await context.new_page()
        try:
            for page_num in range(1, max_pages + 1):
                url = self.CATEGORY_URLS[category].format(page=page_num)
                await page.goto(url, wait_until="domcontentloaded", timeout=self.config.page_timeout)
                await page.wait_for_selector(self.config.list_selector, timeout=self.config.page_timeout)
                html = await page.content()
                soup = BeautifulSoup(html, 'html.parser')
                list_area = soup.select_one(self.config.list_selector)
                if not list_area:
                    continue
                for a in list_area.find_all('a', href=True):
                    href = a['href']
                    if href.startswith('/arti/economy/'):
                        full_url = f"https://www.hani.co.kr{href}"
                        article_urls.add(full_url)
                if len(article_urls) >= min_count:
                    break
        finally:
            await page.close()
            await context.close()
        return list(article_urls)

    async def parse_article(self, page: Page, url: str) -> Optional[Dict[str, Any]]:
        """ê¸°ì‚¬ ìƒì„¸ ì •ë³´ íŒŒì‹± (ë³¸ë¬¸, ì œëª©, ë°œí–‰ì¼, ê¸°ìëª…, ì´ë¯¸ì§€ ë“±)"""
        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=self.config.page_timeout)
            await page.wait_for_selector(self.config.article_selector, timeout=self.config.page_timeout)
            html = await page.content()
            soup = BeautifulSoup(html, 'html.parser')

            # ì œëª©
            title_el = soup.select_one(self.config.title_selector)
            title = title_el.get_text(strip=True) if title_el else None
            if not title or len(title) < self.config.min_title_length:
                return None

            # ë³¸ë¬¸
            article_area = soup.select_one(self.config.article_selector)
            content = None
            if article_area:
                # ê´‘ê³ , ìŠ¤í¬ë¦½íŠ¸ ë“± ë¶ˆí•„ìš” ìš”ì†Œ ì œê±°
                for unwanted in article_area.select("script, style, .ad, .advertisement"):
                    unwanted.decompose()
                content = article_area.get_text(strip=True)
            if not content or len(content) < self.config.min_content_length:
                return None

            # ë°œí–‰ì¼
            date_el = soup.select_one(self.config.date_selector)
            published_at = None
            if date_el:
                # ul > li êµ¬ì¡°ì—ì„œ ë‚ ì§œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                li_tags = date_el.find_all('li')
                for li in li_tags:
                    txt = li.get_text(strip=True)
                    # ë‚ ì§œ í˜•ì‹ì´ í¬í•¨ëœ lië§Œ ì¶”ì¶œ
                    if 'ë“±ë¡' in txt or 'ë°œí–‰' in txt:
                        published_at = txt
                        break

            # ê¸°ìëª…
            author_el = soup.select_one(self.config.author_selector)
            author = author_el.get_text(strip=True) if author_el else None

            # ì´ë¯¸ì§€
            image_el = soup.select_one(self.config.image_selector)
            image_url = image_el['src'] if image_el and image_el.has_attr('src') else None

            return {
                "title": title,
                "url": url,
                "content_full": content,
                "published_at": published_at,
                "author": author,
                "image_url": image_url
            }
        except Exception as e:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
            return None

    async def save_to_supabase(self, articles: List[Dict[str, Any]]) -> int:
        """Supabase articles í…Œì´ë¸”ì— ê¸°ì‚¬ ì €ì¥ (ì¤‘ë³µ URL unique constraint ìë™ ë°©ì§€)"""
        try:
            article_models = []
            for article_dict in articles:
                published_at = article_dict.get('published_at')
                # published_at íŒŒì‹± (ë‚ ì§œ ë¬¸ìì—´ì´ ìˆìœ¼ë©´ datetime ë³€í™˜ ì‹œë„)
                if published_at and isinstance(published_at, str):
                    try:
                        published_at = published_at.replace('ë“±ë¡ ', '').replace('ë°œí–‰ ', '').replace('ìˆ˜ì • ', '').replace('.', '-').replace(' ', 'T')
                        # ì˜ˆ: 2025-07-14T11:48
                        if 'T' in published_at:
                            published_at = published_at + ':00' if len(published_at) == 16 else published_at
                        from datetime import datetime
                        published_at = datetime.fromisoformat(published_at)
                    except Exception:
                        published_at = None
                bias = article_dict.get('bias') or 'center-left'
                article_model = Article(
                    title=article_dict['title'],
                    url=article_dict['url'],
                    category='ê²½ì œ',
                    content_full=article_dict.get('content_full'),
                    published_at=published_at,
                    author=article_dict.get('author'),
                    image_url=article_dict.get('image_url'),
                    bias=bias,
                    media_id=article_dict.get('media_id'),
                )
                article_models.append(article_model)
            saved_count = await self.article_service.save_articles(article_models)
            return saved_count
        except Exception as e:
            # DB ì €ì¥ ì‹¤íŒ¨ ì‹œ 0 ë°˜í™˜
            return 0

    # BaseNewsCrawler ì¶”ìƒ ë©”ì„œë“œ ë”ë¯¸ êµ¬í˜„ (í•„ìˆ˜)
    async def crawl_category(self, *args, **kwargs):
        return []
    async def crawl_all_categories(self, *args, **kwargs):
        return []
    async def run_pipeline(self, *args, **kwargs):
        return None

    async def run(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œâ†’ìƒì„¸ íŒŒì‹±â†’Supabase ì €ì¥, rich í”¼ë“œë°± í¬í•¨ (ìƒì„¸í™”, 30ê°œ ì±„ìš¸ ë•Œê¹Œì§€ ë°˜ë³µ)"""
        console = Console()
        category = HaniCategory.ECONOMY
        articles: list = []
        success_count = 0
        fail_count = 0
        skip_count = 0
        max_pages = 10
        min_count = 30
        console.print("\n[bold cyan]ğŸš€ í•œê²¨ë ˆì‹ ë¬¸ ê²½ì œ í¬ë¡¤ëŸ¬ ì‹œì‘[/bold cyan]")
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True, args=['--no-sandbox', '--disable-dev-shm-usage'])
                # 1. ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ (30ê°œ ì±„ìš¸ ë•Œê¹Œì§€ ë°˜ë³µ)
                console.print(f"[yellow]ğŸ“° {category.value} ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘...[/yellow]")
                article_urls = []
                page_try = 1
                while len(articles) < min_count and page_try <= max_pages:
                    article_urls = await self.fetch_article_list(browser, category, min_count=min_count, max_pages=page_try)
                    # 2. ê¸°ì‚¬ ìƒì„¸ íŒŒì‹± (ì§„í–‰ë¥ /ì„±ê³µ/ì‹¤íŒ¨/ìŠ¤í‚µ ì¹´ìš´íŠ¸)
                    context = await browser.new_context()
                    semaphore = asyncio.Semaphore(5)  # ë™ì‹œì— 5ê°œê¹Œì§€
                    async def parse_one(url):
                        async with semaphore:
                            page = await context.new_page()
                            try:
                                article = await self.parse_article(page, url)
                                if article:
                                    article['media_id'] = await self.get_media_id('í•œê²¨ë ˆì‹ ë¬¸')
                                    article['bias'] = await self.get_media_bias('í•œê²¨ë ˆì‹ ë¬¸')
                                return article
                            finally:
                                await page.close()
                    articles = []
                    success_count = 0
                    fail_count = 0
                    skip_count = 0
                    with Progress(
                        SpinnerColumn(),
                        BarColumn(),
                        TextColumn("{task.description}"),
                        TimeElapsedColumn(),
                        console=console,
                    ) as progress:
                        task = progress.add_task(f"[cyan]{page_try}í˜ì´ì§€ê¹Œì§€ ê¸°ì‚¬ ìƒì„¸ íŒŒì‹± ì¤‘...", total=len(article_urls))
                        tasks = [parse_one(url) for url in article_urls]
                        for f in asyncio.as_completed(tasks):
                            article = await f
                            if article:
                                articles.append(article)
                                success_count += 1
                                if len(articles) >= min_count:
                                    break
                            else:
                                fail_count += 1
                            progress.update(task, advance=1)
                    await context.close()
                    if len(articles) >= min_count:
                        break
                    page_try += 1
                console.print(f"[blue]ğŸ” íŒŒì‹± ì„±ê³µ: {success_count}ê±´, ì‹¤íŒ¨: {fail_count}ê±´, ìŠ¤í‚µ: {skip_count}ê±´[/blue]")
                if not articles:
                    console.print("[red]âŒ ìœ íš¨í•œ ê¸°ì‚¬ íŒŒì‹± ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.[/red]")
                    return
                # 3. Supabase ì €ì¥ (ì¤‘ë³µ URL unique constraintë¡œ ìë™ ë°©ì§€)
                console.print("[yellow]ğŸ’¾ Supabase DB ì €ì¥ ì‹œë„ ì¤‘...[/yellow]")
                try:
                    saved_count = await self.save_to_supabase(articles[:min_count])
                    if saved_count > 0:
                        console.print(f"[bold green]âœ… {saved_count}ê°œ ê¸°ì‚¬ DB ì €ì¥ ì™„ë£Œ (ì¤‘ë³µ ì œì™¸)[/bold green]")
                        if saved_count < len(articles[:min_count]):
                            console.print(f"[yellow]âš ï¸ {len(articles[:min_count])-saved_count}ê°œëŠ” ì´ë¯¸ DBì— ì¡´ì¬í•˜ì—¬ ì €ì¥ë˜ì§€ ì•ŠìŒ[/yellow]")
                    else:
                        console.print("[red]âŒ DB ì €ì¥ ì‹¤íŒ¨ ë˜ëŠ” ì¤‘ë³µ ê¸°ì‚¬ë§Œ ì¡´ì¬[/red]")
                except Exception as e:
                    console.print(f"[red]âŒ DB ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}[/red]")
                console.print(f"[bold magenta]ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ![/bold magenta] (ì´ íŒŒì‹±: {len(articles[:min_count])}ê±´, DB ì €ì¥: {saved_count}ê±´)")
        except Exception as e:
            console.print(f"[red]âŒ í¬ë¡¤ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}[/red]")

    async def get_media_id(self, media_name: str) -> str:
        # ArticleServiceì˜ get_or_create_media í™œìš©
        media_info = await self.article_service.get_or_create_media(media_name)
        if media_info and media_info.get('id'):
            return media_info['id']
        # ì•ˆì „í•œ ê¸°ë³¸ê°’(UUID í˜•ì‹)
        return '00000000-0000-0000-0000-000000000000'

    async def get_media_bias(self, media_name: str) -> str:
        media_info = await self.article_service.get_or_create_media(media_name)
        if media_info and media_info.get('bias'):
            return media_info['bias']
        return 'center-left'

    async def save_articles(self, *args, **kwargs):
        return 0

async def main():
    config = HaniCrawlerConfig()
    crawler = HaniCrawler(config)
    await crawler.run()

# ì§ì ‘ ì‹¤í–‰ ì‹œ: ë¹„ë™ê¸° main í•¨ìˆ˜
if __name__ == "__main__":
    import asyncio
    asyncio.run(main()) 