import asyncio
import json
import logging
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Set, Tuple, Any
from dataclasses import dataclass
from enum import Enum

import aiofiles
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Browser, Page

# Supabase ì—°ë™ì„ ìœ„í•œ import
import sys
sys.path.append(str(Path(__file__).parent.parent))
from app.services.article_service import ArticleService
from app.models.article import Article
from crawler.base_crawler import BaseNewsCrawler

# ë¡œê¹… ì„¤ì • - íŒŒì¼ê³¼ ì½˜ì†” ë¶„ë¦¬
def setup_logging():
    """ë¡œê¹… ì„¤ì •ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    
    # íŒŒì¼ ë¡œê±° (ìƒì„¸ ì •ë³´)
    file_handler = logging.FileHandler(log_dir / "crawler_detailed.log", encoding="utf-8")
    file_handler.setLevel(logging.DEBUG)
    file_formatter = logging.Formatter(
        "%(asctime)s %(name)s %(levelname)s %(message)s"
    )
    file_handler.setFormatter(file_formatter)
    
    # ì½˜ì†” ë¡œê±° (ê°„ë‹¨í•œ ì •ë³´ë§Œ)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter("%(message)s")
    console_handler.setFormatter(console_formatter)
    
    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    logging.basicConfig(
        level=logging.DEBUG,
        handlers=[file_handler, console_handler]
    )

setup_logging()
logger = logging.getLogger(__name__)

class Category(Enum):
    """í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ ì •ì˜"""
    ECONOMY = "ê²½ì œ"
    POLITICS = "ì •ì¹˜"
    NATIONAL = "ì‚¬íšŒ"
    CULTURE = "ë¬¸í™”"
    INTERNATIONAL = "êµ­ì œ"
    SPORTS = "ìŠ¤í¬ì¸ "

@dataclass
class CrawlerConfig:
    """í¬ë¡¤ëŸ¬ ì„¤ì • í´ë˜ìŠ¤"""
    max_more_clicks: int = 10
    articles_per_category: int = 30
    page_timeout: int = 20000
    wait_timeout: int = 2000
    min_content_length: int = 50
    min_title_length: int = 10
    
    # CSS ì…€ë ‰í„°ë“¤
    headline_selector: str = 'a.story-card__headline'
    title_selectors: Optional[List[str]] = None
    content_selectors: Optional[List[str]] = None
    date_selectors: Optional[List[str]] = None
    author_selectors: Optional[List[str]] = None
    image_selectors: Optional[List[str]] = None
    
    def __post_init__(self):
        if self.title_selectors is None:
            self.title_selectors = ['h1', '.article-title', '.story-title']
        if self.content_selectors is None:
            self.content_selectors = [
                '.article-body', '.news-article-body', '.story-content', 
                '.entry-content', '#article-body', '.article-content', 
                '.text-content', '.content-body', '.article-text', '.story-body'
            ]
        if self.date_selectors is None:
            self.date_selectors = [
                "meta[property='article:published_time']", 
                "meta[property='og:article:published_time']", 
                "time", ".published-date", ".article-date"
            ]
        if self.author_selectors is None:
            self.author_selectors = [
                "meta[property='og:article:author']", 
                ".author", ".byline", ".writer", ".reporter"
            ]
        if self.image_selectors is None:
            self.image_selectors = [
                "meta[property='og:image']", 
                "meta[property='twitter:image']", 
                ".article-image img", ".story-image img"
            ]

class ConsoleUI:
    """í„°ë¯¸ë„ UI ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def print_header():
        """í¬ë¡¤ë§ ì‹œì‘ í—¤ë”ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print("\n" + "="*60)
        print("ğŸš€ ì¡°ì„ ì¼ë³´ í¬ë¡¤ëŸ¬ ì‹œì‘")
        print("="*60)
    
    @staticmethod
    def print_category_start(category: str):
        """ì¹´í…Œê³ ë¦¬ ì‹œì‘ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print(f"\nğŸ“° {category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¤‘...")
    
    @staticmethod
    def print_progress(category: str, current: int, target: int, total_articles: int):
        """ì§„í–‰ ìƒí™©ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        progress = min(100, int(current / target * 100))
        bar = "â–ˆ" * (progress // 5) + "â–‘" * (20 - progress // 5)
        print(f"   {bar} {current}/{target} ({progress}%) - ì´ {total_articles}ê°œ ê¸°ì‚¬")
    
    @staticmethod
    def print_category_complete(category: str, count: int):
        """ì¹´í…Œê³ ë¦¬ ì™„ë£Œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print(f"âœ… {category}: {count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
    
    @staticmethod
    def print_summary(total_articles: int, filepath: str):
        """ìµœì¢… ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print("\n" + "="*60)
        print(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ {total_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {filepath}")
        print("="*60 + "\n")

class ArticleExtractor:
    """ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ í´ë˜ìŠ¤"""
    
    def __init__(self, config: CrawlerConfig):
        self.config = config
    
    async def extract_article_content(self, page: Page, url: str) -> Optional[Dict[str, Any]]:
        """ê¸°ì‚¬ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=self.config.page_timeout)
            await page.wait_for_timeout(500)
            
            html = await page.content()
            soup = BeautifulSoup(html, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = self._extract_title(soup)
            if not title:
                logger.debug(f"ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}")
                return None
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = self._extract_content(soup)
            if not content or len(content.strip()) < self.config.min_content_length:
                logger.debug(f"ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}")
                return None
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            published_at = self._extract_published_date(soup)
            author = self._extract_author(soup)
            image_url = self._extract_image_url(soup)
            
            return {
                "title": title,
                "url": url,
                "content_full": content,
                "published_at": published_at or datetime.now().isoformat(),
                "author": author,
                "image_url": image_url
            }
            
        except Exception as e:
            logger.debug(f"ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {str(e)}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if self.config.title_selectors is None:
            return None
        for selector in self.config.title_selectors:
            el = soup.select_one(selector)
            if el:
                title = el.get_text(strip=True)
                if len(title) >= self.config.min_title_length:
                    return title
        return None
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if self.config.content_selectors is None:
            return ""
        for selector in self.config.content_selectors:
            el = soup.select_one(selector)
            if el:
                # ê´‘ê³  ë° ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for unwanted in el.select("script, style, .ad, .advertisement"):
                    unwanted.decompose()
                content = el.get_text(strip=True)
                if len(content) > 100:
                    return content
        return ""
    
    def _extract_published_date(self, soup: BeautifulSoup) -> Optional[str]:
        """ë°œí–‰ì¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if self.config.date_selectors is None:
            return None
        for selector in self.config.date_selectors:
            if selector.startswith("meta"):
                meta = soup.select_one(selector)
                if meta:
                    content = meta.get("content")
                    if content and isinstance(content, str):
                        return content
            else:
                el = soup.select_one(selector)
                if el:
                    time_str = el.get("datetime") or el.get_text(strip=True)
                    if time_str and isinstance(time_str, str):
                        return time_str
        return None
    
    def _extract_author(self, soup: BeautifulSoup) -> Optional[str]:
        """ì‘ì„±ìë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if self.config.author_selectors is None:
            return None
        for selector in self.config.author_selectors:
            if selector.startswith("meta"):
                meta = soup.select_one(selector)
                if meta:
                    content = meta.get("content")
                    if content and isinstance(content, str):
                        return content
            else:
                el = soup.select_one(selector)
                if el:
                    return el.get_text(strip=True)
        return None
    
    def _extract_image_url(self, soup: BeautifulSoup) -> Optional[str]:
        """ì´ë¯¸ì§€ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        if self.config.image_selectors is None:
            return None
        for selector in self.config.image_selectors:
            if selector.startswith("meta"):
                meta = soup.select_one(selector)
                if meta:
                    content = meta.get("content")
                    if content and isinstance(content, str):
                        return content
            else:
                img = soup.select_one(selector)
                if img:
                    src = img.get("src")
                    if src and isinstance(src, str):
                        return src
        return None

class ChosunCrawler(BaseNewsCrawler):
    """ì¡°ì„ ì¼ë³´ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""
    
    CATEGORY_URLS = {
        Category.ECONOMY: "https://www.chosun.com/economy/",
        Category.POLITICS: "https://www.chosun.com/politics/",
        Category.NATIONAL: "https://www.chosun.com/national/",
        Category.CULTURE: "https://www.chosun.com/culture-style/",
        Category.INTERNATIONAL: "https://www.chosun.com/international/",
        Category.SPORTS: "https://www.chosun.com/sports/"
    }
    
    def __init__(self, config: CrawlerConfig):
        self.config = config
        self.extractor = ArticleExtractor(config)
        self.collected_titles: Set[str] = set()
        self.ui = ConsoleUI()
        self.article_service = ArticleService()
    
    async def crawl_category(self, browser: Browser, category: Category) -> List[Dict[str, Any]]:
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        articles = []
        context = await browser.new_context()
        page = await context.new_page()
        
        try:
            self.ui.print_category_start(category.value)
            base_url = self.CATEGORY_URLS[category]
            
            await page.goto(base_url, wait_until="domcontentloaded", timeout=self.config.page_timeout)
            await page.wait_for_timeout(1000)
            
            # ë”ë³´ê¸° ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì¶©ë¶„í•œ ê¸°ì‚¬ ìˆ˜ì§‘
            await self._load_more_articles(page, category)
            
            # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ ë° í¬ë¡¤ë§
            links_and_titles = await self._extract_article_links(page, category)
            
            # ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ
            articles = await self._extract_articles(page, category, links_and_titles)
            
            self.ui.print_category_complete(category.value, len(articles))
            
        except Exception as e:
            logger.error(f"{category.value} í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
        finally:
            await page.close()
            await context.close()
        
        return articles
    
    async def _load_more_articles(self, page: Page, category: Category) -> None:
        """ë”ë³´ê¸° ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì¤‘ë³µ ì—†ëŠ” ê¸°ì‚¬ 30ê°œê°€ ëª¨ì¼ ë•Œê¹Œì§€ ë°˜ë³µí•©ë‹ˆë‹¤."""
        collected_urls = set()
        prev_count = 0
        for click_count in range(self.config.max_more_clicks):
            # í˜„ì¬ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ (ì¤‘ë³µ ì—†ëŠ” set)
            current_links = await self._get_current_article_links(page)
            for url, _ in current_links:
                normalized_url = url.split('?')[0] if '?' in url else url
                collected_urls.add(normalized_url)
            
            # ëª©í‘œ ê¸°ì‚¬ ìˆ˜ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
            if len(collected_urls) >= self.config.articles_per_category:
                break
            
            # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­
            more_btn = await page.query_selector("#load-more-stories")
            if more_btn:
                await more_btn.click()
                await page.wait_for_timeout(self.config.wait_timeout * 2)
                await page.wait_for_load_state("networkidle", timeout=10000)
            else:
                break
            
            # ë”ë³´ê¸°ë¥¼ ëˆŒëŸ¬ë„ ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ í•˜ë‚˜ë„ ì¶”ê°€ë˜ì§€ ì•Šìœ¼ë©´ ì¤‘ë‹¨
            if len(collected_urls) == prev_count:
                break
            prev_count = len(collected_urls)
    
    async def _get_current_article_links(self, page: Page) -> List[Tuple[str, str]]:
        """í˜„ì¬ í˜ì´ì§€ì˜ ê¸°ì‚¬ ë§í¬ì™€ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        html = await page.content()
        soup = BeautifulSoup(html, "html.parser")
        
        links_and_titles = []
        for a in soup.select(self.config.headline_selector):
            href = a.get('href')
            title = a.get_text(strip=True)
            
            if isinstance(href, list):
                href = href[0] if href else None
                
            if href and title and re.search(r'/20[0-9]{2}/[01][0-9]/[0-3][0-9]/', href):
                if href.startswith('/'):
                    full_url = f"https://www.chosun.com{href}"
                else:
                    full_url = href
                links_and_titles.append((full_url, title))
        
        return links_and_titles
    
    async def _extract_article_links(self, page: Page, category: Category) -> List[Tuple[str, str]]:
        """ìµœì¢… ê¸°ì‚¬ ë§í¬ì™€ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        links_and_titles = await self._get_current_article_links(page)
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        unique_links = []
        seen_urls = set()
        
        for url, title in links_and_titles:
            # URL ì •ê·œí™” (ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°)
            normalized_url = url.split('?')[0] if '?' in url else url
            
            if normalized_url not in seen_urls:
                seen_urls.add(normalized_url)
                unique_links.append((url, title))
        
        return unique_links
    
    async def _extract_articles(self, page: Page, category: Category, links_and_titles: List[Tuple[str, str]]) -> List[Dict[str, Any]]:
        """ê¸°ì‚¬ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        articles = []
        processed_count = 0
        skipped_duplicates = 0
        
        for i, (url, title) in enumerate(links_and_titles):
            if processed_count >= self.config.articles_per_category:
                break
            
            # ì¤‘ë³µ ì œëª© ì²´í¬
            if title in self.collected_titles:
                skipped_duplicates += 1
                continue
            
            try:
                article = await self.extractor.extract_article_content(page, url)
                if article:
                    # ì¶”ì¶œëœ ì œëª©ìœ¼ë¡œ ë‹¤ì‹œ ì¤‘ë³µ ì²´í¬
                    extracted_title = article['title']
                    if extracted_title in self.collected_titles:
                        skipped_duplicates += 1
                        continue
                    
                    article["category"] = category.value
                    article["source"] = "chosun"
                    articles.append(article)
                    self.collected_titles.add(extracted_title)
                    processed_count += 1
                    
                    # ì§„í–‰ ìƒí™© ì¶œë ¥
                    self.ui.print_progress(category.value, processed_count, self.config.articles_per_category, len(articles))
                    
                else:
                    logger.debug(f"{category.value}: ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨ - {url}")
                    
            except Exception as e:
                logger.error(f"{category.value} ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨: {url} - {e}")
        
        return articles
    
    async def crawl_all_categories(self, test_mode: bool = False) -> List[Dict[str, Any]]:
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤. test_mode=Trueë©´ êµ­ì œ/ìŠ¤í¬ì¸ ëŠ” ì œì™¸í•©ë‹ˆë‹¤."""
        all_articles = []
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True, 
                args=['--no-sandbox', '--disable-dev-shm-usage']
            )
            
            try:
                for category in Category:
                    if test_mode and category in [Category.INTERNATIONAL, Category.SPORTS]:
                        continue
                    articles = await self.crawl_category(browser, category)
                    all_articles.extend(articles)
                    
            finally:
                await browser.close()
        
        return all_articles
    
    async def save_articles(self, articles: List[Dict[str, Any]]) -> str:
        """ê¸°ì‚¬ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        today = datetime.now().strftime("%Y%m%d")
        filename = f"chosun_{today}.jsonl"
        raw_dir = Path("data/raw")
        raw_dir.mkdir(parents=True, exist_ok=True)
        filepath = raw_dir / filename
        
        async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
            for article in articles:
                await f.write(json.dumps(article, ensure_ascii=False) + '\n')
        
        return str(filepath)
    
    async def save_articles_to_db(self, articles: List[Dict[str, Any]]) -> int:
        """ê¸°ì‚¬ë¥¼ Supabase ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            # Article ëª¨ë¸ë¡œ ë³€í™˜
            article_models = []
            for article_dict in articles:
                # published_at íŒŒì‹±
                published_at = None
                if article_dict.get('published_at'):
                    try:
                        if isinstance(article_dict['published_at'], str):
                            published_at = datetime.fromisoformat(article_dict['published_at'].replace('Z', '+00:00'))
                        else:
                            published_at = article_dict['published_at']
                    except:
                        published_at = None
                
                article_model = Article(
                    title=article_dict['title'],
                    url=article_dict['url'],
                    category=article_dict['category'],
                    content_full=article_dict.get('content_full'),
                    published_at=published_at,
                    author=article_dict.get('author'),
                    image_url=article_dict.get('image_url'),
                    bias="center"  # ê¸°ë³¸ê°’
                )
                article_models.append(article_model)
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            saved_count = await self.article_service.save_articles(article_models)
            return saved_count
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # í™˜ê²½ë³€ìˆ˜ ì²´í¬
        import os
        if not os.getenv("SUPABASE_URL") or not os.getenv("SUPABASE_ANON_KEY"):
            print("âš ï¸  ê²½ê³ : Supabase í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   apps/backend/.env íŒŒì¼ì— SUPABASE_URLê³¼ SUPABASE_ANON_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
            print("   ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì„ ê±´ë„ˆë›°ê³  íŒŒì¼ ì €ì¥ë§Œ ì§„í–‰í•©ë‹ˆë‹¤.")
        
        config = CrawlerConfig()
        crawler = ChosunCrawler(config)
        
        # UI í—¤ë” ì¶œë ¥
        crawler.ui.print_header()
        
        # ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ (6ê°œ ì¹´í…Œê³ ë¦¬)
        articles = await crawler.crawl_all_categories(test_mode=False)
        
        if articles:
            # íŒŒì¼ ì €ì¥
            filepath = await crawler.save_articles(articles)
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ëœ ê²½ìš°ì—ë§Œ)
            if os.getenv("SUPABASE_URL") and os.getenv("SUPABASE_ANON_KEY"):
                print("\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
                try:
                    saved_count = await crawler.save_articles_to_db(articles)
                    print(f"âœ… {saved_count}ê°œ ê¸°ì‚¬ê°€ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
                    print("   íŒŒì¼ ì €ì¥ì€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                print("\nâš ï¸  ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤. (í™˜ê²½ë³€ìˆ˜ ë¯¸ì„¤ì •)")
            
            # ìµœì¢… ìš”ì•½ ì¶œë ¥
            crawler.ui.print_summary(len(articles), filepath)
        else:
            print("âŒ í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
        raise

if __name__ == "__main__":
    asyncio.run(main()) 