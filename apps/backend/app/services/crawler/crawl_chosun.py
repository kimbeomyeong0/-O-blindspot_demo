import os
import asyncio
from typing import List, Dict
from bs4 import BeautifulSoup
from datetime import datetime
from playwright.async_api import async_playwright
import logging
from .crawler_config import (
    CATEGORY_URLS, 
    BROWSER_ARGS, 
    ARTICLE_SELECTORS, 
    MORE_BUTTON_SELECTORS, 
    CONTENT_SELECTORS
)

logger = logging.getLogger(__name__)

def clean_text(text: str) -> str:
    """ÌÖçÏä§Ìä∏ Ï†ïÎ¶¨"""
    if not text:
        return ""
    return " ".join(text.strip().split())

def extract_summary_excerpt(content: str, max_length: int = 200) -> str:
    """Î≥∏Î¨∏ÏóêÏÑú ÏöîÏïΩ Ï∂îÏ∂ú"""
    if not content or content == "Î≥∏Î¨∏ÏùÑ Ï∂îÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§.":
        return ""
    
    # Ï≤´ 200Ïûê Ï†ïÎèÑ Ï∂îÏ∂ú
    summary = content[:max_length].strip()
    
    # Î¨∏Ïû• Îã®ÏúÑÎ°ú ÏûêÎ•¥Í∏∞
    if len(content) > max_length:
        last_period = summary.rfind('.')
        if last_period > max_length * 0.7:  # 70% Ïù¥ÏÉÅÏóêÏÑú ÎßàÏπ®ÌëúÍ∞Ä ÏûàÏúºÎ©¥ Í±∞Í∏∞ÏÑú ÏûêÎ•¥Í∏∞
            summary = summary[:last_period + 1]
        else:
            summary += "..."
    
    return summary

async def extract_article_content(page, article_url: str) -> str:
    """Ï°∞ÏÑ†ÏùºÎ≥¥ Í∏∞ÏÇ¨ Î≥∏Î¨∏ Ï∂îÏ∂ú"""
    try:
        await page.goto(article_url, wait_until="domcontentloaded", timeout=20000)
        await page.wait_for_timeout(500)
        
        html = await page.content()
        soup = BeautifulSoup(html, 'html.parser')
        
        content = ""
        for selector in CONTENT_SELECTORS:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = clean_text(content_elem.get_text())
                if len(content) > 100:
                    break
        
        if not content or len(content.strip()) < 50:
            content = "Î≥∏Î¨∏ÏùÑ Ï∂îÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§."
            
        return content
        
    except Exception as e:
        logger.error(f"Ï°∞ÏÑ†ÏùºÎ≥¥ Î≥∏Î¨∏ Ï∂îÏ∂ú Ïã§Ìå® - {article_url}: {e}")
        return "Î≥∏Î¨∏ÏùÑ Ï∂îÏ∂úÌï† Ïàò ÏóÜÏäµÎãàÎã§."

async def get_articles() -> List[Dict]:
    """Ï°∞ÏÑ†ÏùºÎ≥¥ Í∏∞ÏÇ¨ ÏàòÏßë"""
    all_articles = []
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=BROWSER_ARGS
        )
        
        for category, urls in CATEGORY_URLS.items():
            try:
                print(f"üîç Ï°∞ÏÑ†ÏùºÎ≥¥ {category} Ïπ¥ÌÖåÍ≥†Î¶¨ ÌÅ¨Î°§ÎßÅ Ï§ë...")
                
                count = 0
                target_count = 30
                processed_urls = set()
                
                for url_idx, url in enumerate(urls):
                    if count >= target_count:
                        break
                        
                    try:
                        print(f"üìÑ Ï°∞ÏÑ†ÏùºÎ≥¥ {category} URL {url_idx + 1}/{len(urls)} Ï≤òÎ¶¨ Ï§ë...")
                        
                        page = await browser.new_page()
                        
                        await page.set_extra_http_headers({
                            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                            'Cache-Control': 'no-cache'
                        })
                        
                        # Î∂àÌïÑÏöîÌïú Î¶¨ÏÜåÏä§ Ï∞®Îã®
                        await page.route('**/*.{png,jpg,jpeg,gif,svg,ico,webp,css,woff,woff2,ttf,eot}', 
                                       lambda route: route.abort())
                        
                        await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                        
                        click_count = 0
                        max_clicks = 35
                        
                        while count < target_count and click_count < max_clicks:
                            html = await page.content()
                            soup = BeautifulSoup(html, "html.parser")
                            
                            nodes = []
                            for selector in ARTICLE_SELECTORS:
                                nodes = soup.select(selector)
                                if nodes:
                                    if click_count == 0:
                                        print(f"üìÑ Ï°∞ÏÑ†ÏùºÎ≥¥ {category} URL {url_idx + 1}ÏóêÏÑú '{selector}' ÏÖÄÎ†âÌÑ∞Î°ú {len(nodes)}Í∞ú ÏöîÏÜå Î∞úÍ≤¨")
                                    break
                            
                            if not nodes:
                                print(f"‚ùå Ï°∞ÏÑ†ÏùºÎ≥¥ {category} URL {url_idx + 1}: Í∏∞ÏÇ¨ Î™©Î°ùÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§")
                                break
                            
                            new_articles_found = 0
                            for node in nodes:
                                if count >= target_count:
                                    break
                                    
                                try:
                                    link = node.get('href')
                                    title = clean_text(node.get_text())
                                    
                                    if not link or not title or len(title) < 3:
                                        continue
                                    
                                    if isinstance(link, list):
                                        link = link[0] if link else ""
                                    
                                    # ÏÉÅÎåÄ Í≤ΩÎ°úÎ•º Ï†àÎåÄ Í≤ΩÎ°úÎ°ú Î≥ÄÌôò
                                    if link.startswith('/'):
                                        link = f"https://www.chosun.com{link}"
                                    elif not link.startswith('http'):
                                        link = f"https://www.chosun.com/{link}"
                                    
                                    # Ï§ëÎ≥µ Ï†úÍ±∞
                                    if link in processed_urls:
                                        continue
                                    
                                    processed_urls.add(link)
                                    
                                    # Î≥∏Î¨∏ Ï∂îÏ∂ú
                                    print(f"üìÑ [{count+1}/{target_count}] {title} - Î≥∏Î¨∏ Ï∂îÏ∂ú Ï§ë...")
                                    content = await extract_article_content(page, article_url=link)
                                    summary_excerpt = extract_summary_excerpt(content)
                                    
                                    # Supabase articles ÌÖåÏù¥Î∏î ÌòïÏãùÏóê ÎßûÏ∂∞ Î∞òÌôò
                                    article = {
                                        "issue_id": None,
                                        "media_id": "chosun",
                                        "title": title,
                                        "summary_excerpt": summary_excerpt,
                                        "url": link,
                                        "bias": "right",
                                        "category": category,
                                        "published_at": datetime.now().isoformat(),
                                        "author": None
                                    }
                                    
                                    all_articles.append(article)
                                    count += 1
                                    new_articles_found += 1
                                    
                                    print(f"‚úÖ [{count}/{target_count}] {title[:50]}... (ÏöîÏïΩ {len(summary_excerpt)}Ïûê)")
                                    
                                except Exception as e:
                                    print(f"‚ùå Ï°∞ÏÑ†ÏùºÎ≥¥ {category} Í∏∞ÏÇ¨ Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {e}")
                                    continue
                            
                            if new_articles_found == 0 and click_count > 0:
                                print(f"  üìÑ Ï°∞ÏÑ†ÏùºÎ≥¥ {category} URL {url_idx + 1}: ÏÉàÎ°úÏö¥ Í∏∞ÏÇ¨Í∞Ä ÏóÜÏñ¥ Ï§ëÎã®")
                                break
                            
                            # ÎçîÎ≥¥Í∏∞ Î≤ÑÌäº Ï∞æÍ∏∞ Î∞è ÌÅ¥Î¶≠
                            try:
                                more_button = None
                                for selector in MORE_BUTTON_SELECTORS:
                                    try:
                                        more_button = page.locator(selector).first
                                        if await more_button.is_visible():
                                            break
                                    except:
                                        continue
                                
                                if more_button and await more_button.is_visible():
                                    print(f"  üîÑ Ï°∞ÏÑ†ÏùºÎ≥¥ {category} URL {url_idx + 1}: ÎçîÎ≥¥Í∏∞ Î≤ÑÌäº ÌÅ¥Î¶≠ ({click_count + 1}Î≤àÏß∏)")
                                    await more_button.click()
                                    click_count += 1
                                    await page.wait_for_timeout(4000)
                                else:
                                    # Ïä§ÌÅ¨Î°§ ÏãúÎèÑ
                                    print(f"  üîÑ Ï°∞ÏÑ†ÏùºÎ≥¥ {category} URL {url_idx + 1}: Ïä§ÌÅ¨Î°§ ÏãúÎèÑ ({click_count + 1}Î≤àÏß∏)")
                                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                                    await page.wait_for_timeout(3000)
                                    
                                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                                    await page.wait_for_timeout(2000)
                                    
                                    new_html = await page.content()
                                    new_soup = BeautifulSoup(new_html, "html.parser")
                                    new_nodes = []
                                    for selector in ARTICLE_SELECTORS:
                                        new_nodes = new_soup.select(selector)
                                        if new_nodes:
                                            break
                                    
                                    if len(new_nodes) <= len(nodes):
                                        print(f"  üìÑ Ï°∞ÏÑ†ÏùºÎ≥¥ {category} URL {url_idx + 1}: Í∏∞ÏÇ¨ ÎçîÎ≥¥Í∏∞ Î≤ÑÌäºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏñ¥ Ï§ëÎã®")
                                        break
                                    
                                    click_count += 1
                                    
                            except Exception as e:
                                print(f"  ‚ùå Ï°∞ÏÑ†ÏùºÎ≥¥ {category} URL {url_idx + 1}: ÎçîÎ≥¥Í∏∞ Î≤ÑÌäº ÌÅ¥Î¶≠ Ï§ë Ïò§Î•ò: {e}")
                                break
                        
                        await page.close()
                        print(f"‚úÖ Ï°∞ÏÑ†ÏùºÎ≥¥ {category} URL {url_idx + 1}ÏóêÏÑú {click_count}Î≤à ÌÅ¥Î¶≠ ÏôÑÎ£å")
                        
                    except Exception as e:
                        print(f"‚ùå Ï°∞ÏÑ†ÏùºÎ≥¥ {category} URL {url_idx + 1} Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò: {e}")
                        if 'page' in locals():
                            await page.close()
                        continue
                
                print(f"‚úÖ Ï°∞ÏÑ†ÏùºÎ≥¥ {category}ÏóêÏÑú {count}Í∞ú Í∏∞ÏÇ¨ ÏàòÏßë ÏôÑÎ£å")
                
            except Exception as e:
                print(f"‚ùå Error crawling Ï°∞ÏÑ†ÏùºÎ≥¥ {category}: {e}")
                if 'page' in locals():
                    await page.close()
                continue
        
        await browser.close()
    
    print(f"‚úÖ Ï°∞ÏÑ†ÏùºÎ≥¥ÏóêÏÑú Ï¥ù {len(all_articles)}Í∞ú Í∏∞ÏÇ¨ ÏàòÏßë ÏôÑÎ£å")
    return all_articles

if __name__ == "__main__":
    asyncio.run(get_articles()) 