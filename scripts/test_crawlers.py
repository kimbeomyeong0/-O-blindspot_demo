#!/usr/bin/env python3
"""
í…ŒìŠ¤íŠ¸ìš© í¬ë¡¤ëŸ¬
ê° ì–¸ë¡ ì‚¬ì—ì„œ ì¹´í…Œê³ ë¦¬ë³„ë¡œ 1ê°œì”©ë§Œ í¬ë¡¤ë§í•˜ì—¬ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸
"""

import asyncio
import sys
import os
from datetime import datetime
from typing import List, Dict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from apps.backend.app.services.crawler.crawl_chosun import get_articles as get_chosun_articles
from apps.backend.app.services.crawler.crawler_config import CATEGORY_URLS

async def test_chosun_crawler() -> List[Dict]:
    """ì¡°ì„ ì¼ë³´ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§"""
    print("=" * 60)
    print("ğŸ” ì¡°ì„ ì¼ë³´ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 60)
    
    # ê¸°ì¡´ í¬ë¡¤ëŸ¬ í•¨ìˆ˜ë¥¼ ì„ì‹œë¡œ ìˆ˜ì •í•˜ì—¬ 1ê°œì”©ë§Œ í¬ë¡¤ë§
    original_crawler = get_chosun_articles
    
    # í…ŒìŠ¤íŠ¸ìš© í¬ë¡¤ëŸ¬ í•¨ìˆ˜ ìƒì„±
    async def test_get_chosun_articles():
        """í…ŒìŠ¤íŠ¸ìš© ì¡°ì„ ì¼ë³´ í¬ë¡¤ëŸ¬ - ì¹´í…Œê³ ë¦¬ë³„ 1ê°œì”©ë§Œ"""
        from apps.backend.app.services.crawler.crawl_chosun import (
            extract_article_content, clean_text, extract_summary_excerpt
        )
        from apps.backend.app.services.crawler.crawler_config import (
            BROWSER_ARGS, ARTICLE_SELECTORS, MORE_BUTTON_SELECTORS, CONTENT_SELECTORS
        )
        from bs4 import BeautifulSoup
        from playwright.async_api import async_playwright
        from datetime import datetime
        
        all_articles = []
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=BROWSER_ARGS
            )
            
            for category, urls in CATEGORY_URLS.items():
                try:
                    print(f"ğŸ“° ì¡°ì„ ì¼ë³´ {category} ì¹´í…Œê³ ë¦¬ í…ŒìŠ¤íŠ¸ ì¤‘...")
                    
                    # ê° í˜ì´ì§€ë³„ë¡œ í…ŒìŠ¤íŠ¸ (ì²« ë²ˆì§¸ ê¸°ì‚¬ë§Œ)
                    for page_idx, url in enumerate(urls):
                        print(f"   ğŸ“„ í˜ì´ì§€ {page_idx + 1}/{len(urls)}: {url}")
                        
                        page = await browser.new_page()
                        
                        await page.set_extra_http_headers({
                            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                            'Cache-Control': 'no-cache'
                        })
                        
                        # ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨
                        await page.route('**/*.{png,jpg,jpeg,gif,svg,ico,webp,css,woff,woff2,ttf,eot}', 
                                       lambda route: route.abort())
                        
                        await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                        
                        html = await page.content()
                        soup = BeautifulSoup(html, "html.parser")
                        
                        # ì²« ë²ˆì§¸ ê¸°ì‚¬ë§Œ ì°¾ê¸°
                        article_found = False
                        for selector in ARTICLE_SELECTORS:
                            nodes = soup.select(selector)
                            if nodes:
                                print(f"      âœ… '{selector}' ì…€ë ‰í„°ë¡œ {len(nodes)}ê°œ ê¸°ì‚¬ ë°œê²¬")
                                
                                # ì²« ë²ˆì§¸ ê¸°ì‚¬ë§Œ ì²˜ë¦¬
                                node = nodes[0]
                                try:
                                    link = node.get('href')
                                    title = clean_text(node.get_text())
                                    
                                    if not link or not title or len(title) < 3:
                                        print(f"      âŒ ì²« ë²ˆì§¸ ê¸°ì‚¬ ì •ë³´ ë¶€ì¡±: link={link}, title={title}")
                                        continue
                                    
                                    if isinstance(link, list):
                                        link = link[0] if link else ""
                                    
                                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                                    if link.startswith('/'):
                                        link = f"https://www.chosun.com{link}"
                                    elif not link.startswith('http'):
                                        link = f"https://www.chosun.com/{link}"
                                    
                                    print(f"      ğŸ“„ ê¸°ì‚¬ ì œëª©: {title}")
                                    print(f"      ğŸ”— ê¸°ì‚¬ URL: {link}")
                                    
                                    # ë³¸ë¬¸ ì¶”ì¶œ
                                    print(f"      ğŸ“– ë³¸ë¬¸ ì¶”ì¶œ ì¤‘...")
                                    content = await extract_article_content(page, article_url=link)
                                    summary_excerpt = extract_summary_excerpt(content)
                                    
                                    article = {
                                        "issue_id": None,
                                        "media_id": "chosun",
                                        "title": title,
                                        "summary_excerpt": summary_excerpt,
                                        "url": link,
                                        "bias": "right",
                                        "category": category,
                                        "published_at": datetime.now().isoformat(),
                                        "author": None,
                                        "page": page_idx + 1  # í˜ì´ì§€ ì •ë³´ ì¶”ê°€
                                    }
                                    
                                    all_articles.append(article)
                                    article_found = True
                                    print(f"      âœ… {category} í˜ì´ì§€ {page_idx + 1} í…ŒìŠ¤íŠ¸ ì™„ë£Œ (ìš”ì•½ {len(summary_excerpt)}ì)")
                                    break
                                    
                                except Exception as e:
                                    print(f"      âŒ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                                    continue
                                
                                if article_found:
                                    break
                        
                        if not article_found:
                            print(f"      âŒ {category} í˜ì´ì§€ {page_idx + 1}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        
                        await page.close()
                        
                        # ëª¨ë“  í˜ì´ì§€ë¥¼ í…ŒìŠ¤íŠ¸í•˜ë¯€ë¡œ break ì œê±°
                        # if article_found:
                        #     break
                    
                    print(f"   âœ… {category} ì¹´í…Œê³ ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                    
                except Exception as e:
                    print(f"âŒ ì¡°ì„ ì¼ë³´ {category} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
                    if 'page' in locals():
                        await page.close()
                    continue
            
            await browser.close()
        
        return all_articles
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    articles = await test_get_chosun_articles()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š ì¡°ì„ ì¼ë³´ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("=" * 60)
    
    for article in articles:
        page_info = f" (í˜ì´ì§€ {article['page']})" if 'page' in article else ""
        print(f"ğŸ“° {article['category']}{page_info}: {article['title'][:50]}...")
        print(f"   ğŸ”— {article['url']}")
        print(f"   ğŸ“ ìš”ì•½: {article['summary_excerpt'][:100]}...")
        print()
    
    print(f"âœ… ì¡°ì„ ì¼ë³´ì—ì„œ ì´ {len(articles)}ê°œ ê¸°ì‚¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    return articles

async def test_all_crawlers():
    """ëª¨ë“  í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ Blindspot í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    all_results = {}
    
    # ì¡°ì„ ì¼ë³´ í…ŒìŠ¤íŠ¸
    try:
        chosun_articles = await test_chosun_crawler()
        all_results['chosun'] = chosun_articles
    except Exception as e:
        print(f"âŒ ì¡°ì„ ì¼ë³´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        all_results['chosun'] = []
    
    # ë‹¤ë¥¸ ì–¸ë¡ ì‚¬ í¬ë¡¤ëŸ¬ê°€ ì¶”ê°€ë˜ë©´ ì—¬ê¸°ì— ì¶”ê°€
    # ì˜ˆ: all_results['hani'] = await test_hani_crawler()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    total_articles = 0
    for media, articles in all_results.items():
        print(f"ğŸ“° {media.upper()}: {len(articles)}ê°œ ê¸°ì‚¬")
        total_articles += len(articles)
    
    print(f"\nâœ… ì´ {total_articles}ê°œ ê¸°ì‚¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print(f"â° ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return all_results

if __name__ == "__main__":
    print("ğŸ¯ Blindspot í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ë„êµ¬")
    print("ê° ì–¸ë¡ ì‚¬ì—ì„œ ì¹´í…Œê³ ë¦¬ë³„ë¡œ 1ê°œì”©ë§Œ í¬ë¡¤ë§í•˜ì—¬ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
    print()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = asyncio.run(test_all_crawlers())
    
    print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!") 